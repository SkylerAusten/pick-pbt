{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b33b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import hypothesis as hp\n",
    "from hypothesis import strategies as st\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from typing import List, Optional, Set, Union\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from hypothesis_pick import (\n",
    "    find_disagreements,\n",
    "    find_stronger_weaker,\n",
    "    infer_implications,\n",
    ")\n",
    "\n",
    "import platform\n",
    "import sys\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "import os\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f4ce0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure PICK: PBT settings\n",
    "FREEFORM_SPEC: bool = False\n",
    "PROBLEM: str = \"dpll\"\n",
    "MODEL_NAME: str = \"gemini-2.5-flash\"\n",
    "TEMPERATURE: float = 0.2\n",
    "MAX_OUTPUT_TOKENS: int = None\n",
    "TIMEOUT_SEC: int = 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "801df790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run timestamp (UTC): 20260119_033543Z\n",
      "Run log path: logs\\dpll_20260119_033543Z.json\n"
     ]
    }
   ],
   "source": [
    "# Define path to log file\n",
    "RUN_TIMESTAMP: str = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%SZ\")\n",
    "LOGS_DIR: Path = Path(\"logs\")\n",
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RUN_LOG_PATH: Path = LOGS_DIR / f\"{PROBLEM}_{RUN_TIMESTAMP}.json\"\n",
    "\n",
    "print(f\"Run timestamp (UTC): {RUN_TIMESTAMP}\")\n",
    "print(f\"Run log path: {RUN_LOG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "389d4fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are helping me generate property-based tests (PBT) in Python.\n",
      "My test framework will be Hypothesis.\n",
      "\n",
      "Problem specification:\n",
      "<background>\n",
      "This problem is a SAT solver for formulas in Conjunctive Normal Form (CNF) using the classic\n",
      "DPLL (Davis–Putnam–Logemann–Loveland) backtracking algorithm.\n",
      "\n",
      "Key ideas:\n",
      "- A CNF formula is a conjunction (AND) of clauses.\n",
      "- A clause is a disjunction (OR) of literals.\n",
      "- A literal is either a variable x (positive) or its negation ¬x (negative).\n",
      "\n",
      "The solver applies deterministic simplifications (unit propagation and pure-literal elimination)\n",
      "and then branches on a chosen literal to search for a satisfying assignment.\n",
      "\n",
      "Important nuance for this repo:\n",
      "The provided instance files include tokens like \"-0\". In Python, int(\"-0\") == 0,\n",
      "so the sign information is lost if literals are represented as integers.\n",
      "This implementation uses a custom Literal type to preserve the sign of variable 0.\n",
      "</background>\n",
      "\n",
      "<custom_data_types>\n",
      "Type aliases (conceptual):\n",
      "\n",
      "- Var: int\n",
      "\t- Non-negative integer variable identifier.\n",
      "\t- Variables are typically 0..N.\n",
      "\n",
      "- Literal:\n",
      "\t- Fields:\n",
      "\t\t- var: Var  (must be >= 0)\n",
      "\t\t- negated: bool  (True means negated literal ¬var)\n",
      "\t- Methods:\n",
      "\t\t- negate() -> Literal  (toggles negated)\n",
      "\t\t- __str__() -> str     (\"-<var>\" if negated else \"<var>\")\n",
      "\t- Rationale:\n",
      "\t\t- Preserves the distinction between \"0\" and \"-0\".\n",
      "\n",
      "- LiteralLike: Union[Literal, str, int]\n",
      "\t- Input convenience type for parsing/normalization.\n",
      "\t- Note: int cannot represent \"-0\" distinctly; use str \"-0\" or Literal(0, True).\n",
      "\n",
      "- Clause: frozenset[Literal]\n",
      "\t- Represents a disjunction of literals.\n",
      "\t- Empty clause (frozenset()) represents a contradiction.\n",
      "\n",
      "- CNF: tuple[Clause, ...]\n",
      "\t- Represents a conjunction of clauses.\n",
      "\t- Empty CNF (tuple()) represents a tautology / trivially satisfiable.\n",
      "\n",
      "- Model: dict[Var, bool]\n",
      "\t- A (partial or full) assignment from variables to boolean values.\n",
      "\t- Interpretation:\n",
      "\t\t- If model[v] == True then variable v is True.\n",
      "\t\t- If model[v] == False then variable v is False.\n",
      "\t\t- A literal evaluates True iff it matches the variable’s assignment and its negation.\n",
      "</custom_data_types>\n",
      "\n",
      "<helper_functions>\n",
      "The implementation includes helper functions for parsing, normalization, simplification,\n",
      "and DPLL heuristics:\n",
      "\n",
      "Parsing / normalization:\n",
      "- parse_literal(x: LiteralLike) -> Literal\n",
      "\t- Accepts Literal, a token string like \"3\" or \"-3\" or \"-0\", or an integer.\n",
      "\t- Raises ValueError on invalid strings, TypeError on unsupported input types.\n",
      "\n",
      "- make_clause(lits: Iterable[LiteralLike]) -> Clause\n",
      "\t- Converts an iterable of literal-like inputs into a Clause (frozenset[Literal]).\n",
      "\n",
      "- make_cnf(clauses: Iterable[Iterable[LiteralLike] | Clause]) -> CNF\n",
      "\t- Converts an iterable of clauses (either already frozenset[Literal] or iterables of literals)\n",
      "\t\tinto a CNF tuple.\n",
      "\n",
      "- parse_instances_text(text: str) -> List[CNF]\n",
      "\t- Parses the repo’s instance format:\n",
      "\t\t- Each non-empty line is a clause (space-separated literal tokens).\n",
      "\t\t- Blank lines separate distinct SAT problems.\n",
      "\n",
      "- load_instances(path: Union[str, Path]) -> List[CNF]\n",
      "\t- Reads a text file and returns parsed CNF problems.\n",
      "\n",
      "Core DPLL mechanics:\n",
      "- _apply_literal(formula: CNF, lit_true: Literal) -> Optional[CNF]\n",
      "\t- Simplifies a CNF given that lit_true is set to True:\n",
      "\t\t- Removes satisfied clauses (those containing lit_true)\n",
      "\t\t- Removes falsified literal occurrences (removes ¬lit_true from clauses)\n",
      "\t\t- Returns None if an empty clause is produced (conflict)\n",
      "\n",
      "- _assign(model: Model, lit_true: Literal) -> bool\n",
      "\t- Updates the model so that lit_true is True.\n",
      "\t- Returns False if the assignment contradicts the model.\n",
      "\n",
      "- _unit_propagate(formula: CNF, model: Model) -> Optional[CNF]\n",
      "\t- Repeatedly finds unit clauses (size 1) and forces their literal True.\n",
      "\t- Returns simplified CNF or None on contradiction.\n",
      "\n",
      "- _pure_literals(formula: CNF, model: Model) -> List[Literal]\n",
      "\t- Returns literals whose variables appear with only one polarity in the remaining CNF.\n",
      "\n",
      "- _eliminate_pure_literals(formula: CNF, model: Model) -> Optional[CNF]\n",
      "\t- Repeatedly assigns pure literals True and simplifies.\n",
      "\n",
      "- _choose_branch_literal(formula: CNF, model: Model) -> Literal\n",
      "\t- Chooses an unassigned literal to branch on.\n",
      "\t- Heuristic: pick a smallest clause first; within it, pick a stable minimal literal.\n",
      "</helper_functions>\n",
      "\n",
      "<function_signature>\n",
      "Primary entrypoints:\n",
      "\n",
      "1) SAT solver:\n",
      "def dpll(\n",
      "\t\tformula: Iterable[Iterable[LiteralLike] | Clause] | CNF,\n",
      "\t\tmodel: Optional[Mapping[Var, bool]] = None,\n",
      ") -> Optional[Model]\n",
      "\n",
      "2) Convenience predicate:\n",
      "def is_satisfiable(\n",
      "\t\tformula: Iterable[Iterable[LiteralLike] | Clause] | CNF,\n",
      ") -> bool\n",
      "\n",
      "3) CNF evaluator:\n",
      "def evaluate_cnf(\n",
      "\t\tformula: Iterable[Iterable[LiteralLike] | Clause] | CNF,\n",
      "\t\tmodel: Mapping[Var, bool],\n",
      ") -> bool\n",
      "</function_signature>\n",
      "\n",
      "<function_details>\n",
      "dpll(formula, model=None) -> Optional[Model]\n",
      "\n",
      "Inputs:\n",
      "- formula: a CNF formula.\n",
      "\t- Preferred representation: CNF = tuple[frozenset[Literal], ...]\n",
      "\t- Convenience representations are accepted and normalized:\n",
      "\t\t- A list/iterable of clauses, where each clause is:\n",
      "\t\t\t- a frozenset[Literal], or\n",
      "\t\t\t- an iterable of LiteralLike values (Literal, str token, or int)\n",
      "\t- IMPORTANT: if variable 0 is negated, represent it as \"-0\" (string) or Literal(0, True).\n",
      "\n",
      "- model: optional partial assignment mapping var->bool.\n",
      "\t- If provided, the solver first simplifies the CNF under those assignments.\n",
      "\n",
      "Outputs:\n",
      "- Returns a satisfying assignment dict {var: bool} if the formula is satisfiable.\n",
      "- Returns None if the formula is unsatisfiable.\n",
      "\n",
      "Algorithm:\n",
      "1) Apply initial model (if provided) by simplifying the CNF.\n",
      "2) Base cases:\n",
      "\t - Empty CNF (no clauses): SAT; return current model.\n",
      "\t - CNF contains an empty clause: UNSAT; return None.\n",
      "3) Deterministic simplifications (repeat until fixed point):\n",
      "\t - Unit propagation (forced literals from unit clauses)\n",
      "\t - Pure-literal elimination\n",
      "\t - If a contradiction is reached during simplification: return None.\n",
      "4) Branching:\n",
      "\t - Choose a literal using _choose_branch_literal.\n",
      "\t - Recurse on the CNF with that literal set True; if SAT, return model.\n",
      "\t - Otherwise recurse with the negation set True; if SAT, return model.\n",
      "\t - If both branches fail: return None.\n",
      "\n",
      "is_satisfiable(formula) -> bool\n",
      "- Returns True iff dpll(formula) is not None.\n",
      "\n",
      "evaluate_cnf(formula, model) -> bool\n",
      "- Returns True iff every clause has at least one literal that evaluates True under the model.\n",
      "- Variables missing from the model are treated as unknown and do not satisfy a literal.\n",
      "\t(So a clause with all literals unassigned/false is unsatisfied.)\n",
      "</function_details>\n",
      "\n",
      "<hypothesis_guidance>\n",
      "Guidance for generating Hypothesis inputs (CNFs) for property-based tests.\n",
      "\n",
      "1) Represent literals\n",
      "- Prefer using Literal(var, negated) directly in strategies.\n",
      "- If you generate from tokens/strings, remember:\n",
      "\t- \"-0\" must be a string (or a Literal), NOT an int.\n",
      "\n",
      "2) Small random CNFs (brute-force cross-check feasible)\n",
      "- Choose small variable counts (e.g., 0..6) and small clause sizes (e.g., 0..4).\n",
      "- Strategy outline:\n",
      "\t- n_vars = st.integers(min_value=0, max_value=6)\n",
      "\t- var = st.integers(min_value=0, max_value=n_vars-1)\n",
      "\t- lit = st.builds(Literal, var, st.booleans())\n",
      "\t- clause = st.sets(lit, min_size=0, max_size=4).map(frozenset)\n",
      "\t- cnf = st.lists(clause, min_size=0, max_size=10).map(tuple)\n",
      "\n",
      "3) Guaranteed SAT CNFs (\"passing\" inputs)\n",
      "- Construct a witness assignment first, then ensure each clause is satisfied by construction.\n",
      "- Recipe:\n",
      "\t- Generate a model witness: witness[v] = st.booleans() for v in vars.\n",
      "\t- For each clause, generate some random literals, then force at least one literal’s polarity\n",
      "\t\tto match the witness so it evaluates True.\n",
      "- Property to check:\n",
      "\t- model = dpll(cnf)\n",
      "\t- assert model is not None and evaluate_cnf(cnf, model)\n",
      "\n",
      "4) Guaranteed UNSAT CNFs (\"failing\" inputs)\n",
      "- Embed an unsatisfiable kernel, then add arbitrary extra clauses (which cannot make it SAT).\n",
      "- Minimal kernel:\n",
      "\t- (x) AND (¬x)\n",
      "\t- In this implementation, you can use x=0 to stress the \"-0\" handling:\n",
      "\t\t- {Literal(0, False)} and {Literal(0, True)}\n",
      "- Property to check:\n",
      "\t- assert dpll(cnf) is None\n",
      "\n",
      "5) Metamorphic / invariance checks\n",
      "- Reordering clauses/literals must not change satisfiable vs unsatisfiable.\n",
      "- Adding a clause that is already satisfied by a known model should not change satisfiability.\n",
      "- Adding a duplicate clause should not change satisfiability.\n",
      "</hypothesis_guidance>\n",
      "\n",
      "Labeled spec expectations (freeform_spec=false):\n",
      "- <background>: high-level context, definitions, and rules.\n",
      "- <custom_data_types>: any custom types, encodings, invariants, examples.\n",
      "- <function_signature>: names, parameters, return types (as text), and meaning.\n",
      "- <function_details>: precise behavior, preconditions, postconditions, errors.\n",
      "\n",
      "Task:\n",
      "- Produce 15 distinct properties of the target function(s).\n",
      "- Each property MUST be testable via random data generation (Hypothesis).\n",
      "- Phrase each property precisely using quantifiers like 'for all' / 'there exists' and include any necessary preconditions.\n",
      "- Include edge cases, invariants, metamorphic properties, and error/exception behavior when relevant.\n",
      "- Do NOT include any code.\n",
      "\n",
      "Output requirements (STRICT):\n",
      "- Output MUST be valid JSON and NOTHING ELSE.\n",
      "- Output MUST be exactly ONE JSON object matching one of the schemas below.\n",
      "\n",
      "Schema A (success):\n",
      "{\n",
      "  \"result\": [\n",
      "    <property description string>,\n",
      "    <property description string>,\n",
      "    ...\n",
      "  ],\n",
      "  \"explanation\": null\n",
      "}\n",
      "\n",
      "Schema B (insufficient/unknown):\n",
      "{\n",
      "  \"result\": null,\n",
      "  \"explanation\": <string describing what is missing / ambiguous and what to add>\n",
      "}\n",
      "\n",
      "Additional constraints:\n",
      "- Each property in result is a string describing one property statement.\n",
      "- Do not wrap in Markdown. Do not include code blocks.\n"
     ]
    }
   ],
   "source": [
    "# Read in the problem specification file\n",
    "def _read_problem_spec(*, problem: str, freeform_spec: bool) -> str:\n",
    "    \"\"\"Load the user-provided spec text for a given problem.\n",
    "\n",
    "    Files:\n",
    "      - if freeform_spec=True:  <problem>/freeform_spec.md\n",
    "      - if freeform_spec=False: <problem>/labeled_spec.xml   (already XML)\n",
    "    \"\"\"\n",
    "    filename = \"freeform_spec.md\" if freeform_spec else \"labeled_spec.xml\"\n",
    "    candidates = [\n",
    "        Path(\"problems\") / problem / filename,\n",
    "        Path(problem) / filename,\n",
    "        Path(\".\") / problem / filename\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            return p.read_text(encoding=\"utf-8\")\n",
    "    raise FileNotFoundError(f\"Couldn't find {filename!r} for problem={problem!r}; tried: {candidates}\")\n",
    "\n",
    "# Build the property generation prompt\n",
    "def build_property_gen_prompt(\n",
    "    *,\n",
    "    problem: str,\n",
    "    freeform_spec: bool,\n",
    "    num_properties: int = 15,\n",
    ") -> str:\n",
    "    \"\"\"Build the prompt sent to Gemini to elicit properties in STRICT JSON.\"\"\"\n",
    "    problem_spec = _read_problem_spec(problem=problem, freeform_spec=freeform_spec).strip()\n",
    "\n",
    "    spec_expectations = (\n",
    "        \"Freeform spec expectations (freeform_spec=true):\\n\"\n",
    "        \"- The user text should include background/problem statement.\\n\"\n",
    "        \"- It should clearly name the target function(s) and what they do.\\n\"\n",
    "        \"- It should describe inputs/outputs, constraints, and edge cases.\\n\"\n",
    "        \"- It should define any custom data types/encodings and examples if relevant.\"\n",
    "        if freeform_spec\n",
    "        else\n",
    "        \"Labeled spec expectations (freeform_spec=false):\\n\"\n",
    "        \"- <background>: high-level context, definitions, and rules.\\n\"\n",
    "        \"- <custom_data_types>: any custom types, encodings, invariants, examples.\\n\"\n",
    "        \"- <function_signature>: names, parameters, return types (as text), and meaning.\\n\"\n",
    "        \"- <function_details>: precise behavior, preconditions, postconditions, errors.\"\n",
    "    )\n",
    "\n",
    "    return \"\\n\".join(\n",
    "        [\n",
    "            \"You are helping me generate property-based tests (PBT) in Python.\",\n",
    "            \"My test framework will be Hypothesis.\",\n",
    "            \"\",\n",
    "            \"Problem specification:\",\n",
    "            problem_spec,\n",
    "            \"\",\n",
    "            spec_expectations,\n",
    "            \"\",\n",
    "            \"Task:\",\n",
    "            f\"- Produce {num_properties} distinct properties of the target function(s).\",\n",
    "            \"- Each property MUST be testable via random data generation (Hypothesis).\",\n",
    "            \"- Phrase each property precisely using quantifiers like 'for all' / 'there exists' and include any necessary preconditions.\",\n",
    "            \"- Include edge cases, invariants, metamorphic properties, and error/exception behavior when relevant.\",\n",
    "            \"- Do NOT include any code.\",\n",
    "            \"\",\n",
    "            \"Output requirements (STRICT):\",\n",
    "            \"- Output MUST be valid JSON and NOTHING ELSE.\",\n",
    "            \"- Output MUST be exactly ONE JSON object matching one of the schemas below.\",\n",
    "            \"\",\n",
    "            \"Schema A (success):\",\n",
    "            \"{\",\n",
    "            \"  \\\"result\\\": [\",\n",
    "            \"    <property description string>,\",\n",
    "            \"    <property description string>,\",\n",
    "            \"    ...\",\n",
    "            \"  ],\",\n",
    "            \"  \\\"explanation\\\": null\",\n",
    "            \"}\",\n",
    "            \"\",\n",
    "            \"Schema B (insufficient/unknown):\",\n",
    "            \"{\",\n",
    "            \"  \\\"result\\\": null,\",\n",
    "            \"  \\\"explanation\\\": <string describing what is missing / ambiguous and what to add>\",\n",
    "            \"}\",\n",
    "            \"\",\n",
    "            \"Additional constraints:\",\n",
    "            \"- Each property in result is a string describing one property statement.\",\n",
    "            \"- Do not wrap in Markdown. Do not include code blocks.\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "PROMPT = build_property_gen_prompt(problem=PROBLEM, freeform_spec=FREEFORM_SPEC)\n",
    "print(PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfb43506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define JSON schema for properties response\n",
    "# Note: Using Google Genai Schema format, not standard JSON Schema.\n",
    "# Each property is now just a string description\n",
    "PROPERTIES_RESPONSE_SCHEMA = {\n",
    "    \"type\": \"OBJECT\",\n",
    "    \"properties\": {\n",
    "        \"result\": {\n",
    "            \"type\": \"ARRAY\",\n",
    "            \"items\": {\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            \"description\": \"Array of property description strings, or null if spec is insufficient\",\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"explanation\": {\n",
    "            \"type\": \"STRING\",\n",
    "            \"description\": \"Explanation of what is missing/ambiguous if result is null, otherwise null\",\n",
    "            \"nullable\": True\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"result\", \"explanation\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "884db197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are helping me generate property-based tests (PBT) in Python.\n",
      "My test framework will be Hypothesis.\n",
      "\n",
      "Problem name: dpll\n",
      "\n",
      "Problem specification:\n",
      "<background>\n",
      "This problem is a SAT solver for formulas in Conjunctive Normal Form (CNF) using the classic\n",
      "DPLL (Davis–Putnam–Logemann–Loveland) backtracking algorithm.\n",
      "\n",
      "Key ideas:\n",
      "- A CNF formula is a conjunction (AND) of clauses.\n",
      "- A clause is a disjunction (OR) of literals.\n",
      "- A literal is either a variable x (positive) or its negation ¬x (negative).\n",
      "\n",
      "The solver applies deterministic simplifications (unit propagation and pure-literal elimination)\n",
      "and then branches on a chosen literal to search for a satisfying assignment.\n",
      "\n",
      "Important nuance for this repo:\n",
      "The provided instance files include tokens like \"-0\". In Python, int(\"-0\") == 0,\n",
      "so the sign information is lost if literals are represented as integers.\n",
      "This implementation uses a custom Literal type to preserve the sign of variable 0.\n",
      "</background>\n",
      "\n",
      "<custom_data_types>\n",
      "Type aliases (conceptual):\n",
      "\n",
      "- Var: int\n",
      "\t- Non-negative integer variable identifier.\n",
      "\t- Variables are typically 0..N.\n",
      "\n",
      "- Literal:\n",
      "\t- Fields:\n",
      "\t\t- var: Var  (must be >= 0)\n",
      "\t\t- negated: bool  (True means negated literal ¬var)\n",
      "\t- Methods:\n",
      "\t\t- negate() -> Literal  (toggles negated)\n",
      "\t\t- __str__() -> str     (\"-<var>\" if negated else \"<var>\")\n",
      "\t- Rationale:\n",
      "\t\t- Preserves the distinction between \"0\" and \"-0\".\n",
      "\n",
      "- LiteralLike: Union[Literal, str, int]\n",
      "\t- Input convenience type for parsing/normalization.\n",
      "\t- Note: int cannot represent \"-0\" distinctly; use str \"-0\" or Literal(0, True).\n",
      "\n",
      "- Clause: frozenset[Literal]\n",
      "\t- Represents a disjunction of literals.\n",
      "\t- Empty clause (frozenset()) represents a contradiction.\n",
      "\n",
      "- CNF: tuple[Clause, ...]\n",
      "\t- Represents a conjunction of clauses.\n",
      "\t- Empty CNF (tuple()) represents a tautology / trivially satisfiable.\n",
      "\n",
      "- Model: dict[Var, bool]\n",
      "\t- A (partial or full) assignment from variables to boolean values.\n",
      "\t- Interpretation:\n",
      "\t\t- If model[v] == True then variable v is True.\n",
      "\t\t- If model[v] == False then variable v is False.\n",
      "\t\t- A literal evaluates True iff it matches the variable’s assignment and its negation.\n",
      "</custom_data_types>\n",
      "\n",
      "<helper_functions>\n",
      "The implementation includes helper functions for parsing, normalization, simplification,\n",
      "and DPLL heuristics:\n",
      "\n",
      "Parsing / normalization:\n",
      "- parse_literal(x: LiteralLike) -> Literal\n",
      "\t- Accepts Literal, a token string like \"3\" or \"-3\" or \"-0\", or an integer.\n",
      "\t- Raises ValueError on invalid strings, TypeError on unsupported input types.\n",
      "\n",
      "- make_clause(lits: Iterable[LiteralLike]) -> Clause\n",
      "\t- Converts an iterable of literal-like inputs into a Clause (frozenset[Literal]).\n",
      "\n",
      "- make_cnf(clauses: Iterable[Iterable[LiteralLike] | Clause]) -> CNF\n",
      "\t- Converts an iterable of clauses (either already frozenset[Literal] or iterables of literals)\n",
      "\t\tinto a CNF tuple.\n",
      "\n",
      "- parse_instances_text(text: str) -> List[CNF]\n",
      "\t- Parses the repo’s instance format:\n",
      "\t\t- Each non-empty line is a clause (space-separated literal tokens).\n",
      "\t\t- Blank lines separate distinct SAT problems.\n",
      "\n",
      "- load_instances(path: Union[str, Path]) -> List[CNF]\n",
      "\t- Reads a text file and returns parsed CNF problems.\n",
      "\n",
      "Core DPLL mechanics:\n",
      "- _apply_literal(formula: CNF, lit_true: Literal) -> Optional[CNF]\n",
      "\t- Simplifies a CNF given that lit_true is set to True:\n",
      "\t\t- Removes satisfied clauses (those containing lit_true)\n",
      "\t\t- Removes falsified literal occurrences (removes ¬lit_true from clauses)\n",
      "\t\t- Returns None if an empty clause is produced (conflict)\n",
      "\n",
      "- _assign(model: Model, lit_true: Literal) -> bool\n",
      "\t- Updates the model so that lit_true is True.\n",
      "\t- Returns False if the assignment contradicts the model.\n",
      "\n",
      "- _unit_propagate(formula: CNF, model: Model) -> Optional[CNF]\n",
      "\t- Repeatedly finds unit clauses (size 1) and forces their literal True.\n",
      "\t- Returns simplified CNF or None on contradiction.\n",
      "\n",
      "- _pure_literals(formula: CNF, model: Model) -> List[Literal]\n",
      "\t- Returns literals whose variables appear with only one polarity in the remaining CNF.\n",
      "\n",
      "- _eliminate_pure_literals(formula: CNF, model: Model) -> Optional[CNF]\n",
      "\t- Repeatedly assigns pure literals True and simplifies.\n",
      "\n",
      "- _choose_branch_literal(formula: CNF, model: Model) -> Literal\n",
      "\t- Chooses an unassigned literal to branch on.\n",
      "\t- Heuristic: pick a smallest clause first; within it, pick a stable minimal literal.\n",
      "</helper_functions>\n",
      "\n",
      "<function_signature>\n",
      "Primary entrypoints:\n",
      "\n",
      "1) SAT solver:\n",
      "def dpll(\n",
      "\t\tformula: Iterable[Iterable[LiteralLike] | Clause] | CNF,\n",
      "\t\tmodel: Optional[Mapping[Var, bool]] = None,\n",
      ") -> Optional[Model]\n",
      "\n",
      "2) Convenience predicate:\n",
      "def is_satisfiable(\n",
      "\t\tformula: Iterable[Iterable[LiteralLike] | Clause] | CNF,\n",
      ") -> bool\n",
      "\n",
      "3) CNF evaluator:\n",
      "def evaluate_cnf(\n",
      "\t\tformula: Iterable[Iterable[LiteralLike] | Clause] | CNF,\n",
      "\t\tmodel: Mapping[Var, bool],\n",
      ") -> bool\n",
      "</function_signature>\n",
      "\n",
      "<function_details>\n",
      "dpll(formula, model=None) -> Optional[Model]\n",
      "\n",
      "Inputs:\n",
      "- formula: a CNF formula.\n",
      "\t- Preferred representation: CNF = tuple[frozenset[Literal], ...]\n",
      "\t- Convenience representations are accepted and normalized:\n",
      "\t\t- A list/iterable of clauses, where each clause is:\n",
      "\t\t\t- a frozenset[Literal], or\n",
      "\t\t\t- an iterable of LiteralLike values (Literal, str token, or int)\n",
      "\t- IMPORTANT: if variable 0 is negated, represent it as \"-0\" (string) or Literal(0, True).\n",
      "\n",
      "- model: optional partial assignment mapping var->bool.\n",
      "\t- If provided, the solver first simplifies the CNF under those assignments.\n",
      "\n",
      "Outputs:\n",
      "- Returns a satisfying assignment dict {var: bool} if the formula is satisfiable.\n",
      "- Returns None if the formula is unsatisfiable.\n",
      "\n",
      "Algorithm:\n",
      "1) Apply initial model (if provided) by simplifying the CNF.\n",
      "2) Base cases:\n",
      "\t - Empty CNF (no clauses): SAT; return current model.\n",
      "\t - CNF contains an empty clause: UNSAT; return None.\n",
      "3) Deterministic simplifications (repeat until fixed point):\n",
      "\t - Unit propagation (forced literals from unit clauses)\n",
      "\t - Pure-literal elimination\n",
      "\t - If a contradiction is reached during simplification: return None.\n",
      "4) Branching:\n",
      "\t - Choose a literal using _choose_branch_literal.\n",
      "\t - Recurse on the CNF with that literal set True; if SAT, return model.\n",
      "\t - Otherwise recurse with the negation set True; if SAT, return model.\n",
      "\t - If both branches fail: return None.\n",
      "\n",
      "is_satisfiable(formula) -> bool\n",
      "- Returns True iff dpll(formula) is not None.\n",
      "\n",
      "evaluate_cnf(formula, model) -> bool\n",
      "- Returns True iff every clause has at least one literal that evaluates True under the model.\n",
      "- Variables missing from the model are treated as unknown and do not satisfy a literal.\n",
      "\t(So a clause with all literals unassigned/false is unsatisfied.)\n",
      "</function_details>\n",
      "\n",
      "<hypothesis_guidance>\n",
      "Guidance for generating Hypothesis inputs (CNFs) for property-based tests.\n",
      "\n",
      "1) Represent literals\n",
      "- Prefer using Literal(var, negated) directly in strategies.\n",
      "- If you generate from tokens/strings, remember:\n",
      "\t- \"-0\" must be a string (or a Literal), NOT an int.\n",
      "\n",
      "2) Small random CNFs (brute-force cross-check feasible)\n",
      "- Choose small variable counts (e.g., 0..6) and small clause sizes (e.g., 0..4).\n",
      "- Strategy outline:\n",
      "\t- n_vars = st.integers(min_value=0, max_value=6)\n",
      "\t- var = st.integers(min_value=0, max_value=n_vars-1)\n",
      "\t- lit = st.builds(Literal, var, st.booleans())\n",
      "\t- clause = st.sets(lit, min_size=0, max_size=4).map(frozenset)\n",
      "\t- cnf = st.lists(clause, min_size=0, max_size=10).map(tuple)\n",
      "\n",
      "3) Guaranteed SAT CNFs (\"passing\" inputs)\n",
      "- Construct a witness assignment first, then ensure each clause is satisfied by construction.\n",
      "- Recipe:\n",
      "\t- Generate a model witness: witness[v] = st.booleans() for v in vars.\n",
      "\t- For each clause, generate some random literals, then force at least one literal’s polarity\n",
      "\t\tto match the witness so it evaluates True.\n",
      "- Property to check:\n",
      "\t- model = dpll(cnf)\n",
      "\t- assert model is not None and evaluate_cnf(cnf, model)\n",
      "\n",
      "4) Guaranteed UNSAT CNFs (\"failing\" inputs)\n",
      "- Embed an unsatisfiable kernel, then add arbitrary extra clauses (which cannot make it SAT).\n",
      "- Minimal kernel:\n",
      "\t- (x) AND (¬x)\n",
      "\t- In this implementation, you can use x=0 to stress the \"-0\" handling:\n",
      "\t\t- {Literal(0, False)} and {Literal(0, True)}\n",
      "- Property to check:\n",
      "\t- assert dpll(cnf) is None\n",
      "\n",
      "5) Metamorphic / invariance checks\n",
      "- Reordering clauses/literals must not change satisfiable vs unsatisfiable.\n",
      "- Adding a clause that is already satisfied by a known model should not change satisfiability.\n",
      "- Adding a duplicate clause should not change satisfiability.\n",
      "</hypothesis_guidance>\n",
      "\n",
      "Labeled spec expectations (freeform_spec=false):\n",
      "- <background>: high-level context, definitions, and rules.\n",
      "- <custom_data_types>: any custom types, encodings, invariants, examples.\n",
      "- <function_signature>: names, parameters, return types (as text), and meaning.\n",
      "- <function_details>: precise behavior, preconditions, postconditions, errors.\n",
      "\n",
      "Task:\n",
      "1) Identify the target function under test, its argument list, and its return type.\n",
      "2) Propose a standard *property-checking* function signature suitable for Hypothesis.\n",
      "   - It should accept the function under test (as a Callable) and the same inputs that function expects.\n",
      "   - It must return a boolean (True/False) indicating whether the property holds.\n",
      "3) Provide Hypothesis strategies to generate valid inputs for the property-checking function.\n",
      "   - Provide strategies for the *inputs* (not for the Callable itself).\n",
      "   - Respect any constraints/invariants from the spec.\n",
      "   - If some constraints are hard, propose assume()-style preconditions (as strings) or filtering.\n",
      "\n",
      "Output requirements (STRICT):\n",
      "- Output MUST be valid JSON and NOTHING ELSE.\n",
      "- If the spec is insufficient or the inputs are impossible/unsupported to generate with Hypothesis, return:\n",
      "  {\"result\": null, \"explanation\": <string>}.\n",
      "- Otherwise return an object of this shape:\n",
      "  {\n",
      "    \"result\": {\n",
      "      \"function_under_test\": {\"name\": <string>, \"args\": [<string>...], \"returns\": <string>},\n",
      "      \"property_function\": {\"name\": <string>, \"typing\": <string>, \"args\": [<string>...], \"returns\": \"bool\"},\n",
      "      \"strategies\": [\n",
      "        {\"arg\": <string>, \"strategy\": <string>, \"notes\": <string|null>},\n",
      "        ...\n",
      "      ],\n",
      "      \"assumptions\": [<string>...],\n",
      "      \"imports\": [<string>...]\n",
      "    }\n",
      "  }\n",
      "- Provide at most 12 strategy entries.\n",
      "- The values in \"strategy\" should be Python expressions like 'st.integers(...)' as STRINGS (not full code).\n"
     ]
    }
   ],
   "source": [
    "# Build prompt to define hypothesis signature + strategy\n",
    "def build_signature_and_strategies_prompt(\n",
    "    *,\n",
    "    problem: str,\n",
    "    freeform_spec: bool,\n",
    "    max_strategies: int = 12,\n",
    ") -> str:\n",
    "    \"\"\"Build a Gemini prompt that returns a JSON schema for test-harness signature + strategies.\"\"\"\n",
    "    problem_spec = _read_problem_spec(problem=problem, freeform_spec=freeform_spec).strip()\n",
    "\n",
    "    spec_expectations = (\n",
    "        \"Freeform spec expectations (freeform_spec=true):\\n\"\n",
    "        \"- The user text should include background/problem statement.\\n\"\n",
    "        \"- It should clearly name the target function(s) and what they do.\\n\"\n",
    "        \"- It should describe inputs/outputs, constraints, edge cases.\\n\"\n",
    "        \"- It should define any custom data types/encodings and examples if relevant.\"\n",
    "        if freeform_spec\n",
    "        else\n",
    "        \"Labeled spec expectations (freeform_spec=false):\\n\"\n",
    "        \"- <background>: high-level context, definitions, and rules.\\n\"\n",
    "        \"- <custom_data_types>: any custom types, encodings, invariants, examples.\\n\"\n",
    "        \"- <function_signature>: names, parameters, return types (as text), and meaning.\\n\"\n",
    "        \"- <function_details>: precise behavior, preconditions, postconditions, errors.\"\n",
    "    )\n",
    "\n",
    "    return \"\\n\".join(\n",
    "        [\n",
    "            \"You are helping me generate property-based tests (PBT) in Python.\",\n",
    "            \"My test framework will be Hypothesis.\",\n",
    "            \"\",\n",
    "            f\"Problem name: {problem}\",\n",
    "            \"\",\n",
    "            \"Problem specification:\",\n",
    "            problem_spec,\n",
    "            \"\",\n",
    "            spec_expectations,\n",
    "            \"\",\n",
    "            \"Task:\",\n",
    "            \"1) Identify the target function under test, its argument list, and its return type.\",\n",
    "            \"2) Propose a standard *property-checking* function signature suitable for Hypothesis.\",\n",
    "            \"   - It should accept the function under test (as a Callable) and the same inputs that function expects.\",\n",
    "            \"   - It must return a boolean (True/False) indicating whether the property holds.\",\n",
    "            \"3) Provide Hypothesis strategies to generate valid inputs for the property-checking function.\",\n",
    "            \"   - Provide strategies for the *inputs* (not for the Callable itself).\",\n",
    "            \"   - Respect any constraints/invariants from the spec.\",\n",
    "            \"   - If some constraints are hard, propose assume()-style preconditions (as strings) or filtering.\",\n",
    "            \"\",\n",
    "            \"Output requirements (STRICT):\",\n",
    "            \"- Output MUST be valid JSON and NOTHING ELSE.\",\n",
    "            \"- If the spec is insufficient or the inputs are impossible/unsupported to generate with Hypothesis, return:\",\n",
    "            \"  {\\\"result\\\": null, \\\"explanation\\\": <string>}.\",\n",
    "            \"- Otherwise return an object of this shape:\",\n",
    "            \"  {\",\n",
    "            \"    \\\"result\\\": {\",\n",
    "            \"      \\\"function_under_test\\\": {\\\"name\\\": <string>, \\\"args\\\": [<string>...], \\\"returns\\\": <string>},\",\n",
    "            \"      \\\"property_function\\\": {\\\"name\\\": <string>, \\\"typing\\\": <string>, \\\"args\\\": [<string>...], \\\"returns\\\": \\\"bool\\\"},\",\n",
    "            \"      \\\"strategies\\\": [\",\n",
    "            \"        {\\\"arg\\\": <string>, \\\"strategy\\\": <string>, \\\"notes\\\": <string|null>},\",\n",
    "            \"        ...\",\n",
    "            \"      ],\",\n",
    "            \"      \\\"assumptions\\\": [<string>...],\",\n",
    "            \"      \\\"imports\\\": [<string>...]\",\n",
    "            \"    }\",\n",
    "            \"  }\",\n",
    "            f\"- Provide at most {max_strategies} strategy entries.\",\n",
    "            \"- The values in \\\"strategy\\\" should be Python expressions like 'st.integers(...)' as STRINGS (not full code).\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "SIGNATURE_STRATEGY_PROMPT = build_signature_and_strategies_prompt(\n",
    "    problem=PROBLEM,\n",
    "    freeform_spec=FREEFORM_SPEC,\n",
    "    max_strategies=12,\n",
    ")\n",
    "\n",
    "print(SIGNATURE_STRATEGY_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "179025ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_gemini_api_key() -> str:\n",
    "    \"\"\"Load the Gemini API key from a .env file or environment variable.\"\"\"\n",
    "    load_dotenv(dotenv_path=Path(\".env\"), override=False)\n",
    "    key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not key:\n",
    "        raise RuntimeError(\n",
    "            \"GEMINI_API_KEY is missing. Add it to a .env file at the repo root as GEMINI_API_KEY=...\"\n",
    "        )\n",
    "    return key\n",
    "\n",
    "# Call Gemini with JSON response\n",
    "def call_gemini_json(\n",
    "    prompt: str,\n",
    "    *,\n",
    "    model_name: Optional[str] = None,\n",
    "    temperature: float = 0.2,\n",
    "    max_output_tokens: Optional[int] = None,\n",
    "    timeout_s: int = 60,\n",
    "    schema: Optional[dict[str, Any]] = None,\n",
    ") -> Tuple[Any, str]:\n",
    "    \"\"\"Call Gemini with `prompt` and return (parsed_json, raw_text).\n",
    "\n",
    "    This relies on Gemini JSON mode (response_mime_type='application/json').\n",
    "    If JSON parsing fails, the raised error includes the raw response text\n",
    "    (truncated) so you can see what the model actually returned.\n",
    "    \"\"\"\n",
    "    api_key = _load_gemini_api_key()\n",
    "    client = genai.Client(api_key=api_key)\n",
    "\n",
    "    effective_model_name = model_name or os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-flash\")\n",
    "    \n",
    "    generation_config = genai.types.GenerateContentConfig(\n",
    "        temperature=temperature,\n",
    "        response_mime_type=\"application/json\",\n",
    "    )\n",
    "    if max_output_tokens is not None:\n",
    "        generation_config.max_output_tokens = max_output_tokens\n",
    "    if schema is not None:\n",
    "        generation_config.response_schema = schema\n",
    "\n",
    "    resp = client.models.generate_content(\n",
    "        model=effective_model_name,\n",
    "        contents=prompt,\n",
    "        config=generation_config,\n",
    "    )\n",
    "    raw = (resp.text or \"\").strip()\n",
    "\n",
    "    if not raw:\n",
    "        # This often happens when the request is blocked, times out, or returns no text.\n",
    "        # We include `resp` metadata to aid debugging without trying to \"extract\" JSON.\n",
    "        raise ValueError(\n",
    "            \"Model returned an empty response in JSON mode. \"\n",
    "            f\"model={effective_model_name!r}. \"\n",
    "            \"Check for safety blocks, timeouts, or SDK/model compatibility.\"\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        return json.loads(raw), raw\n",
    "    except json.JSONDecodeError as e:\n",
    "        excerpt = raw[:4000]\n",
    "        raise ValueError(\n",
    "            \"Model did not return valid JSON (despite JSON mode). \"\n",
    "            f\"model={effective_model_name!r}. \"\n",
    "            \"Raw model output (truncated):\\n\" + excerpt\n",
    "        ) from e\n",
    "\n",
    "def validate_property_list(properties_json: dict) -> None:\n",
    "    \"\"\"Validate that property list has correct structure.\n",
    "    \n",
    "    Args:\n",
    "        properties_json: The parsed JSON response from Gemini\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If validation fails\n",
    "    \"\"\"\n",
    "    if properties_json is None:\n",
    "        raise ValueError(\"Properties JSON is None\")\n",
    "    \n",
    "    result = properties_json.get(\"result\")\n",
    "    \n",
    "    # If result is null, that's okay (insufficient spec case)\n",
    "    if result is None:\n",
    "        return\n",
    "    \n",
    "    if not isinstance(result, list):\n",
    "        raise ValueError(f\"result must be a list, got {type(result).__name__}\")\n",
    "    \n",
    "    for i, item in enumerate(result):\n",
    "        if not isinstance(item, str):\n",
    "            raise ValueError(\n",
    "                f\"Property {i} must be a string, got {type(item).__name__}: {item}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a2977e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw model output (truncated):\n",
      "{\n",
      "  \"result\": [\n",
      "    \"For any satisfiable CNF `formula`, `dpll(formula)` must return a `Model` `M`, and `evaluate_cnf(formula, M)` must return `True`.\",\n",
      "    \"For any unsatisfiable CNF `formula`, `dpll(formula)` must return `None`.\",\n",
      "    \"For any CNF `formula`, `is_satisfiable(formula)` must return `True` if and only if `dpll(formula)` does not return `None`.\",\n",
      "    \"For an empty CNF (i.e., `tuple()`), `dpll(tuple())` must return a `Model` (e.g., an empty dictionary), and `is_satisfiable(tuple())` must return `True`.\",\n",
      "    \"For any CNF `formula` that contains an empty clause (i.e., `frozenset()`), `dpll(formula)` must return `None`, and `is_satisfiable(formula)` must return `False`.\",\n",
      "    \"If a CNF `formula` contains an empty clause, then for any `model`, `evaluate_cnf(formula, model)` must return `False`.\",\n",
      "    \"For any CNF `formula`, if `formula_shuffled` is a CNF formed by reordering the clauses of `formula`, then `dpll(formula)` and `dpll(formula_shuffled)` must have the same satisfiability status (both `None` or both a `Model`).\",\n",
      "    \"For any CNF `formula`, if `formula_reordered_lits` is a CNF formed by reordering literals within any of its clauses (e.g., by converting `frozenset` to `list` and back), then `dpll(formula)` and `dpll(formula_reordered_lits)` must have the same satisfiability status.\",\n",
      "    \"For any CNF `formula` and any variable `v`, if `formula_with_tautology` is `formula` with the clause `{Literal(v, False), Literal(v, True)}` added, then `dpll(formula)` and `dpll(formula_with_tautology)` must have the same satisfiability status.\",\n",
      "    \"For any satisfiable CNF `formula`, let `M` be a satisfying model returned by `dpll(formula)`. If `clause_satisfied_by_M` is any clause such that `evaluate_cnf(tuple([clause_satisfied_by_M]), M)` is `True`, then `dpll(formula + (clause_satisfied_by_M,))` must also return a satisfying model.\",\n",
      "    \"For any CNF `formula` and any `clause` from `formula`, if `formula_with_duplicate` is `formula` with `clause` appended, ...\n",
      "\n",
      "Parsed JSON:\n",
      "{'explanation': None,\n",
      " 'result': ['For any satisfiable CNF `formula`, `dpll(formula)` must return a '\n",
      "            '`Model` `M`, and `evaluate_cnf(formula, M)` must return `True`.',\n",
      "            'For any unsatisfiable CNF `formula`, `dpll(formula)` must return '\n",
      "            '`None`.',\n",
      "            'For any CNF `formula`, `is_satisfiable(formula)` must return '\n",
      "            '`True` if and only if `dpll(formula)` does not return `None`.',\n",
      "            'For an empty CNF (i.e., `tuple()`), `dpll(tuple())` must return a '\n",
      "            '`Model` (e.g., an empty dictionary), and '\n",
      "            '`is_satisfiable(tuple())` must return `True`.',\n",
      "            'For any CNF `formula` that contains an empty clause (i.e., '\n",
      "            '`frozenset()`), `dpll(formula)` must return `None`, and '\n",
      "            '`is_satisfiable(formula)` must return `False`.',\n",
      "            'If a CNF `formula` contains an empty clause, then for any '\n",
      "            '`model`, `evaluate_cnf(formula, model)` must return `False`.',\n",
      "            'For any CNF `formula`, if `formula_shuffled` is a CNF formed by '\n",
      "            'reordering the clauses of `formula`, then `dpll(formula)` and '\n",
      "            '`dpll(formula_shuffled)` must have the same satisfiability status '\n",
      "            '(both `None` or both a `Model`).',\n",
      "            'For any CNF `formula`, if `formula_reordered_lits` is a CNF '\n",
      "            'formed by reordering literals within any of its clauses (e.g., by '\n",
      "            'converting `frozenset` to `list` and back), then `dpll(formula)` '\n",
      "            'and `dpll(formula_reordered_lits)` must have the same '\n",
      "            'satisfiability status.',\n",
      "            'For any CNF `formula` and any variable `v`, if '\n",
      "            '`formula_with_tautology` is `formula` with the clause '\n",
      "            '`{Literal(v, False), Literal(v, True)}` added, then '\n",
      "            '`dpll(formula)` and `dpll(formula_with_tautology)` must have the '\n",
      "            'same satisfiability status.',\n",
      "            'For any satisfiable CNF `formula`, let `M` be a satisfying model '\n",
      "            'returned by `dpll(formula)`. If `clause_satisfied_by_M` is any '\n",
      "            'clause such that `evaluate_cnf(tuple([clause_satisfied_by_M]), '\n",
      "            'M)` is `True`, then `dpll(formula + (clause_satisfied_by_M,))` '\n",
      "            'must also return a satisfying model.',\n",
      "            'For any CNF `formula` and any `clause` from `formula`, if '\n",
      "            '`formula_with_duplicate` is `formula` with `clause` appended, '\n",
      "            'then `dpll(formula)` and `dpll(formula_with_duplicate)` must have '\n",
      "            'the same satisfiability status.',\n",
      "            'For any CNF `formula` and any partial `model_initial`, if '\n",
      "            '`dpll(formula, model=model_initial)` returns a `Model` '\n",
      "            '`M_result`, then `M_result` must be consistent with '\n",
      "            '`model_initial` (i.e., for all `var` in `model_initial`, '\n",
      "            '`M_result[var]` must equal `model_initial[var]`). If '\n",
      "            '`dpll(formula, model=model_initial)` returns `None`, then no '\n",
      "            'model consistent with `model_initial` can satisfy `formula`.',\n",
      "            'For any CNF `formula` involving variable 0 (e.g., `Literal(0, '\n",
      "            'False)` and `Literal(0, True)`), `dpll(formula)` must correctly '\n",
      "            'determine its satisfiability, and if satisfiable, return a model '\n",
      "            'where variable 0 is correctly assigned.',\n",
      "            'For a CNF `formula` consisting of a single clause `(clause,)` '\n",
      "            'where `clause` contains only one literal `{Literal(v, p)}`, '\n",
      "            '`dpll(formula)` must return a model where `v` is assigned `p`. '\n",
      "            'For `formula = ({Literal(v, False)}, {Literal(v, True)})`, '\n",
      "            '`dpll(formula)` must return `None`.',\n",
      "            'For small CNFs (e.g., up to 4 variables and 6 clauses), the '\n",
      "            'satisfiability result (SAT/UNSAT) returned by `dpll(formula)` '\n",
      "            'must match the result obtained from a brute-force truth table '\n",
      "            'evaluation.']}\n"
     ]
    }
   ],
   "source": [
    "# Call for properties generation \n",
    "properties_json = None\n",
    "properties_raw = \"\"\n",
    "properties_error = None\n",
    "\n",
    "try:\n",
    "    properties_json, properties_raw = call_gemini_json(\n",
    "        PROMPT,\n",
    "        model_name=MODEL_NAME,\n",
    "        temperature=TEMPERATURE,\n",
    "        max_output_tokens=MAX_OUTPUT_TOKENS,\n",
    "        timeout_s=TIMEOUT_SEC,\n",
    "        schema=PROPERTIES_RESPONSE_SCHEMA,\n",
    "    )\n",
    "    # Validate the structure\n",
    "    validate_property_list(properties_json)\n",
    "except Exception as e:\n",
    "    properties_error = f\"{type(e).__name__}: {e}\"\n",
    "\n",
    "print(\"Raw model output (truncated):\")\n",
    "print(properties_raw[:2000] + (\"...\" if len(properties_raw) > 2000 else \"\"))\n",
    "print(\"\\nParsed JSON:\")\n",
    "pprint(properties_json)\n",
    "if properties_error:\n",
    "    print(\"\\nERROR:\")\n",
    "    print(properties_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5cd971d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw model output (truncated):\n",
      "{\n",
      "  \"parameters\": [\n",
      "    {\n",
      "      \"name\": \"N\",\n",
      "      \"type_hint\": \"Any\"\n",
      "    }\n",
      "  ],\n",
      "  \"strategies\": [\n",
      "    {\n",
      "      \"param_name\": \"N\",\n",
      "      \"strategy\": \"st.one_of(st.integers(min_value=1, max_value=3999), st.integers(max_value=0), st.integers(min_value=4000), st.floats(allow_nan=false, allow_infinity=false), st.text(), st.none(), st.booleans())\",\n",
      "      \"notes\": \"Generates valid integers (1-3999) for core functionality tests, out-of-range integers (<1, >3999) for ValueError tests, and various non-integer types (float, string, None, boolean) for TypeError tests. This comprehensive strategy allows a single test function to cover all specified properties, including error handling.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Parsed JSON:\n",
      "{'parameters': [{'name': 'N', 'type_hint': 'Any'}],\n",
      " 'strategies': [{'notes': 'Generates valid integers (1-3999) for core '\n",
      "                          'functionality tests, out-of-range integers (<1, '\n",
      "                          '>3999) for ValueError tests, and various '\n",
      "                          'non-integer types (float, string, None, boolean) '\n",
      "                          'for TypeError tests. This comprehensive strategy '\n",
      "                          'allows a single test function to cover all '\n",
      "                          'specified properties, including error handling.',\n",
      "                 'param_name': 'N',\n",
      "                 'strategy': 'st.one_of(st.integers(min_value=1, '\n",
      "                             'max_value=3999), st.integers(max_value=0), '\n",
      "                             'st.integers(min_value=4000), '\n",
      "                             'st.floats(allow_nan=false, '\n",
      "                             'allow_infinity=false), st.text(), st.none(), '\n",
      "                             'st.booleans())'}]}\n"
     ]
    }
   ],
   "source": [
    "# ===== Generate PBT function signature and strategies from property list =====\n",
    "# This will call Gemini with the spec and ALL properties to get a unified test function signature\n",
    "\n",
    "# Define schema for PBT function signature response\n",
    "PBT_SIGNATURE_SCHEMA = {\n",
    "    \"type\": \"OBJECT\",\n",
    "    \"properties\": {\n",
    "        \"parameters\": {\n",
    "            \"type\": \"ARRAY\",\n",
    "            \"items\": {\n",
    "                \"type\": \"OBJECT\",\n",
    "                \"properties\": {\n",
    "                    \"name\": {\"type\": \"STRING\"},\n",
    "                    \"type_hint\": {\"type\": \"STRING\"},\n",
    "                },\n",
    "                \"required\": [\"name\", \"type_hint\"]\n",
    "            },\n",
    "            \"description\": \"List of parameters for the test_function_pbt function\"\n",
    "        },\n",
    "        \"strategies\": {\n",
    "            \"type\": \"ARRAY\",\n",
    "            \"items\": {\n",
    "                \"type\": \"OBJECT\",\n",
    "                \"properties\": {\n",
    "                    \"param_name\": {\"type\": \"STRING\"},\n",
    "                    \"strategy\": {\"type\": \"STRING\"},\n",
    "                    \"notes\": {\"type\": \"STRING\", \"nullable\": True},\n",
    "                },\n",
    "                \"required\": [\"param_name\", \"strategy\"]\n",
    "            },\n",
    "            \"description\": \"Hypothesis strategies for each parameter\"\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"parameters\", \"strategies\"]\n",
    "}\n",
    "\n",
    "pbt_signature_json = None\n",
    "pbt_signature_raw = \"\"\n",
    "pbt_signature_error = None\n",
    "\n",
    "# Only proceed if we got properties\n",
    "if properties_json and properties_json.get(\"result\"):\n",
    "    property_list = properties_json.get(\"result\", [])\n",
    "    problem_spec_text = _read_problem_spec(problem=PROBLEM, freeform_spec=FREEFORM_SPEC).strip()\n",
    "    \n",
    "    # Build prompt for PBT signature generation\n",
    "    properties_str = \"\\n\".join([f\"{i+1}. {prop}\" for i, prop in enumerate(property_list)])\n",
    "    \n",
    "    PBT_SIGNATURE_PROMPT = '\\n'.join([\n",
    "        \"You are designing a property-based test (PBT) function signature for Hypothesis.\",\n",
    "        \"Output MUST be valid JSON and NOTHING ELSE and match the provided schema.\",\n",
    "        \"\",\n",
    "        \"Problem specification:\",\n",
    "        problem_spec_text,\n",
    "        \"\",\n",
    "        \"Properties to test:\",\n",
    "        properties_str,\n",
    "        \"\",\n",
    "        \"Task:\",\n",
    "        \"1) Determine the parameters needed for a test function that can check ALL these properties.\",\n",
    "        \"   - The function signature will be: def test_function_pbt(<parameters>) -> bool:\",\n",
    "        \"   - Parameters should represent the INPUTS to the function under test (e.g., if testing to_numerals(n), the parameter would be 'n: int').\",\n",
    "        \"   - Include type hints for each parameter.\",\n",
    "        \"2) Provide Hypothesis strategies to generate each parameter.\",\n",
    "        \"   - Strategies should be Python expressions like 'st.integers(min_value=1, max_value=3999)'.\",\n",
    "        \"   - Respect constraints from the spec and properties.\",\n",
    "        \"\",\n",
    "        \"Return a JSON object with:\",\n",
    "        \"- parameters: [{\\\"name\\\": <str>, \\\"type_hint\\\": <str>}, ...]\",\n",
    "        \"- strategies: [{\\\"param_name\\\": <str>, \\\"strategy\\\": <str>, \\\"notes\\\": <str|null>}, ...]\",\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        pbt_signature_json, pbt_signature_raw = call_gemini_json(\n",
    "            PBT_SIGNATURE_PROMPT,\n",
    "            model_name=MODEL_NAME,\n",
    "            temperature=TEMPERATURE,\n",
    "            max_output_tokens=MAX_OUTPUT_TOKENS,\n",
    "            timeout_s=TIMEOUT_SEC,\n",
    "            schema=PBT_SIGNATURE_SCHEMA,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        pbt_signature_error = f\"{type(e).__name__}: {e}\"\n",
    "\n",
    "print(\"Raw model output (truncated):\")\n",
    "print(pbt_signature_raw[:2000] + (\"...\" if len(pbt_signature_raw) > 2000 else \"\"))\n",
    "print(\"\\nParsed JSON:\")\n",
    "pprint(pbt_signature_json)\n",
    "if pbt_signature_error:\n",
    "    print(\"\\nERROR:\")\n",
    "    print(pbt_signature_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "918e2536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Implementation 1: Valid - This implementation performs a comprehensive check for valid inputs, covering roundtrip conversion, type, character validity, length, repetition limits, and specific invalid subtractive pairs. It also verifies the non-increasing order of token values, ensuring adherence to the greedy decomposition algorithm.\n",
      "✅ Implementation 2: Valid - This implementation focuses primarily on error handling for invalid input types and out-of-range integer values, asserting that the correct exceptions (TypeError or ValueError) are raised. It also includes direct assertions for specific, well-known Roman numeral conversions.\n",
      "✅ Implementation 3: Valid - This implementation emphasizes structural correctness of the Roman numeral string, performing extensive substring checks to ensure adherence to repetition limits (e.g., no 'VV', 'IIII') and the absence of invalid two-symbol sequences. It also verifies that any subtractive pairs are among the allowed canonical forms.\n",
      "✅ Implementation 4: Valid - This implementation prioritizes the core functional correctness by asserting the roundtrip property (from_numerals(to_numerals(N)) == N). It also strongly verifies the algorithmic behavior by checking that the parsed token values are strictly non-increasing, reflecting the greedy decomposition requirement.\n",
      "\n",
      "✅ Successfully generated 4 valid implementations!\n",
      "\n",
      "============================================================\n",
      "Generated function body implementations:\n",
      "\n",
      "--- Implementation 1 ---\n",
      "Description: This implementation performs a comprehensive check for valid inputs, covering roundtrip conversion, type, character validity, length, repetition limits, and specific invalid subtractive pairs. It also verifies the non-increasing order of token values, ensuring adherence to the greedy decomposition algorithm.\n",
      "Imports: [\"ROMAN_MAP_VALUES = {\\n    'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000,\\n    'IV': 4, 'IX': 9, 'XL': 40, 'XC': 90, 'CD': 400, 'CM': 900\\n}\", \"ROMAN_SYMBOLS = set('IVXLCDM')\", \"ROMAN_TOKEN_VALUES = [\\n    (1000, 'M'), (900, 'CM'), (500, 'D'), (400, 'CD'), (100, 'C'), (90, 'XC'),\\n    (50, 'L'), (40, 'XL'), (10, 'X'), (9, 'IX'), (5, 'V'), (4, 'IV'), (1, 'I')\\n]\", 'def from_numerals(roman_numeral_string: str) -> int:\\n    if not isinstance(roman_numeral_string, str):\\n        raise TypeError(\"Input must be a string.\")\\n    if not roman_numeral_string:\\n        raise ValueError(\"Input string cannot be empty.\")\\n    total = 0\\n    i = 0\\n    while i < len(roman_numeral_string):\\n        if i + 1 < len(roman_numeral_string):\\n            two_char_symbol = roman_numeral_string[i:i+2]\\n            if two_char_symbol in ROMAN_MAP_VALUES and len(two_char_symbol) == 2:\\n                total += ROMAN_MAP_VALUES[two_char_symbol]\\n                i += 2\\n                continue\\n        one_char_symbol = roman_numeral_string[i]\\n        if one_char_symbol in ROMAN_MAP_VALUES:\\n            total += ROMAN_MAP_VALUES[one_char_symbol]\\n            i += 1\\n        else:\\n            raise ValueError(f\"Invalid Roman numeral symbol: {one_char_symbol}\")\\n    return total', 'def get_token_values(roman_string: str) -> list[int]:\\n    values = []\\n    i = 0\\n    while i < len(roman_string):\\n        matched = False\\n        for val, token in ROMAN_TOKEN_VALUES:\\n            if roman_string.startswith(token, i):\\n                values.append(val)\\n                i += len(token)\\n                matched = True\\n                break\\n        if not matched:\\n            raise ValueError(f\"Could not parse token at index {i} in {roman_string}\")\\n    return values']\n",
      "Body preview (first 200 chars):\n",
      "    print(f\"Testing N={N}\")\n",
      "    result = to_numerals(N)\n",
      "\n",
      "    # Property 2: Type check\n",
      "    assert isinstance(result, str), f\"Expected string, got {type(result)} for N={N}\"\n",
      "\n",
      "    # Property 1: Roundtrip\n",
      "...\n",
      "\n",
      "--- Implementation 2 ---\n",
      "Description: This implementation focuses primarily on error handling for invalid input types and out-of-range integer values, asserting that the correct exceptions (TypeError or ValueError) are raised. It also includes direct assertions for specific, well-known Roman numeral conversions.\n",
      "Imports: [\"ROMAN_MAP_VALUES = {\\n    'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000,\\n    'IV': 4, 'IX': 9, 'XL': 40, 'XC': 90, 'CD': 400, 'CM': 900\\n}\", \"ROMAN_SYMBOLS = set('IVXLCDM')\", \"ROMAN_TOKEN_VALUES = [\\n    (1000, 'M'), (900, 'CM'), (500, 'D'), (400, 'CD'), (100, 'C'), (90, 'XC'),\\n    (50, 'L'), (40, 'XL'), (10, 'X'), (9, 'IX'), (5, 'V'), (4, 'IV'), (1, 'I')\\n]\", 'def from_numerals(roman_numeral_string: str) -> int:\\n    if not isinstance(roman_numeral_string, str):\\n        raise TypeError(\"Input must be a string.\")\\n    if not roman_numeral_string:\\n        raise ValueError(\"Input string cannot be empty.\")\\n    total = 0\\n    i = 0\\n    while i < len(roman_numeral_string):\\n        if i + 1 < len(roman_numeral_string):\\n            two_char_symbol = roman_numeral_string[i:i+2]\\n            if two_char_symbol in ROMAN_MAP_VALUES and len(two_char_symbol) == 2:\\n                total += ROMAN_MAP_VALUES[two_char_symbol]\\n                i += 2\\n                continue\\n        one_char_symbol = roman_numeral_string[i]\\n        if one_char_symbol in ROMAN_MAP_VALUES:\\n            total += ROMAN_MAP_VALUES[one_char_symbol]\\n            i += 1\\n        else:\\n            raise ValueError(f\"Invalid Roman numeral symbol: {one_char_symbol}\")\\n    return total', 'def get_token_values(roman_string: str) -> list[int]:\\n    values = []\\n    i = 0\\n    while i < len(roman_string):\\n        matched = False\\n        for val, token in ROMAN_TOKEN_VALUES:\\n            if roman_string.startswith(token, i):\\n                values.append(val)\\n                i += len(token)\\n                matched = True\\n                break\\n        if not matched:\\n            raise ValueError(f\"Could not parse token at index {i} in {roman_string}\")\\n    return values', \"EXPECTED_VALUES = {\\n        1: 'I', 4: 'IV', 5: 'V', 9: 'IX', 10: 'X', 40: 'XL', 50: 'L', 90: 'XC',\\n        100: 'C', 400: 'CD', 500: 'D', 900: 'CM', 1000: 'M'\\n    }\"]\n",
      "Body preview (first 200 chars):\n",
      "    print(f\"Testing N={N}\")\n",
      "\n",
      "    # Property 9: TypeError for non-integer N\n",
      "    if not isinstance(N, int):\n",
      "        try:\n",
      "            to_numerals(N)\n",
      "            assert False, f\"Expected TypeError for non...\n",
      "\n",
      "--- Implementation 3 ---\n",
      "Description: This implementation emphasizes structural correctness of the Roman numeral string, performing extensive substring checks to ensure adherence to repetition limits (e.g., no 'VV', 'IIII') and the absence of invalid two-symbol sequences. It also verifies that any subtractive pairs are among the allowed canonical forms.\n",
      "Imports: [\"ROMAN_MAP_VALUES = {\\n    'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000,\\n    'IV': 4, 'IX': 9, 'XL': 40, 'XC': 90, 'CD': 400, 'CM': 900\\n}\", \"ROMAN_SYMBOLS = set('IVXLCDM')\", \"ROMAN_TOKEN_VALUES = [\\n    (1000, 'M'), (900, 'CM'), (500, 'D'), (400, 'CD'), (100, 'C'), (90, 'XC'),\\n    (50, 'L'), (40, 'XL'), (10, 'X'), (9, 'IX'), (5, 'V'), (4, 'IV'), (1, 'I')\\n]\", 'def from_numerals(roman_numeral_string: str) -> int:\\n    if not isinstance(roman_numeral_string, str):\\n        raise TypeError(\"Input must be a string.\")\\n    if not roman_numeral_string:\\n        raise ValueError(\"Input string cannot be empty.\")\\n    total = 0\\n    i = 0\\n    while i < len(roman_numeral_string):\\n        if i + 1 < len(roman_numeral_string):\\n            two_char_symbol = roman_numeral_string[i:i+2]\\n            if two_char_symbol in ROMAN_MAP_VALUES and len(two_char_symbol) == 2:\\n                total += ROMAN_MAP_VALUES[two_char_symbol]\\n                i += 2\\n                continue\\n        one_char_symbol = roman_numeral_string[i]\\n        if one_char_symbol in ROMAN_MAP_VALUES:\\n            total += ROMAN_MAP_VALUES[one_char_symbol]\\n            i += 1\\n        else:\\n            raise ValueError(f\"Invalid Roman numeral symbol: {one_char_symbol}\")\\n    return total', 'def get_token_values(roman_string: str) -> list[int]:\\n    values = []\\n    i = 0\\n    while i < len(roman_string):\\n        matched = False\\n        for val, token in ROMAN_TOKEN_VALUES:\\n            if roman_string.startswith(token, i):\\n                values.append(val)\\n                i += len(token)\\n                matched = True\\n                break\\n        if not matched:\\n            raise ValueError(f\"Could not parse token at index {i} in {roman_string}\")\\n    return values']\n",
      "Body preview (first 200 chars):\n",
      "    print(f\"Testing N={N}\")\n",
      "    result = to_numerals(N)\n",
      "\n",
      "    # Property 2: Type check\n",
      "    assert isinstance(result, str), f\"Expected string, got {type(result)} for N={N}\"\n",
      "\n",
      "    # Property 3: Valid char...\n",
      "\n",
      "--- Implementation 4 ---\n",
      "Description: This implementation prioritizes the core functional correctness by asserting the roundtrip property (from_numerals(to_numerals(N)) == N). It also strongly verifies the algorithmic behavior by checking that the parsed token values are strictly non-increasing, reflecting the greedy decomposition requirement.\n",
      "Imports: [\"ROMAN_MAP_VALUES = {\\n    'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000,\\n    'IV': 4, 'IX': 9, 'XL': 40, 'XC': 90, 'CD': 400, 'CM': 900\\n}\", \"ROMAN_SYMBOLS = set('IVXLCDM')\", \"ROMAN_TOKEN_VALUES = [\\n    (1000, 'M'), (900, 'CM'), (500, 'D'), (400, 'CD'), (100, 'C'), (90, 'XC'),\\n    (50, 'L'), (40, 'XL'), (10, 'X'), (9, 'IX'), (5, 'V'), (4, 'IV'), (1, 'I')\\n]\", 'def from_numerals(roman_numeral_string: str) -> int:\\n    if not isinstance(roman_numeral_string, str):\\n        raise TypeError(\"Input must be a string.\")\\n    if not roman_numeral_string:\\n        raise ValueError(\"Input string cannot be empty.\")\\n    total = 0\\n    i = 0\\n    while i < len(roman_numeral_string):\\n        if i + 1 < len(roman_numeral_string):\\n            two_char_symbol = roman_numeral_string[i:i+2]\\n            if two_char_symbol in ROMAN_MAP_VALUES and len(two_char_symbol) == 2:\\n                total += ROMAN_MAP_VALUES[two_char_symbol]\\n                i += 2\\n                continue\\n        one_char_symbol = roman_numeral_string[i]\\n        if one_char_symbol in ROMAN_MAP_VALUES:\\n            total += ROMAN_MAP_VALUES[one_char_symbol]\\n            i += 1\\n        else:\\n            raise ValueError(f\"Invalid Roman numeral symbol: {one_char_symbol}\")\\n    return total', 'def get_token_values(roman_string: str) -> list[int]:\\n    values = []\\n    i = 0\\n    while i < len(roman_string):\\n        matched = False\\n        for val, token in ROMAN_TOKEN_VALUES:\\n            if roman_string.startswith(token, i):\\n                values.append(val)\\n                i += len(token)\\n                matched = True\\n                break\\n        if not matched:\\n            raise ValueError(f\"Could not parse token at index {i} in {roman_string}\")\\n    return values']\n",
      "Body preview (first 200 chars):\n",
      "    print(f\"Testing N={N}\")\n",
      "    result = to_numerals(N)\n",
      "\n",
      "    # Property 2: Type check\n",
      "    assert isinstance(result, str), f\"Expected string, got {type(result)} for N={N}\"\n",
      "\n",
      "    # Property 3: Valid char...\n"
     ]
    }
   ],
   "source": [
    "# ===== Generate test function body implementation =====\n",
    "# Call LLM to generate 4 semantically different implementations\n",
    "\n",
    "# Define schema for function body response\n",
    "FUNCTION_BODY_SCHEMA = {\n",
    "    \"type\": \"OBJECT\",\n",
    "    \"properties\": {\n",
    "        \"implementations\": {\n",
    "            \"type\": \"ARRAY\",\n",
    "            \"items\": {\n",
    "                \"type\": \"OBJECT\",\n",
    "                \"properties\": {\n",
    "                    \"body\": {\n",
    "                        \"type\": \"STRING\",\n",
    "                        \"description\": \"The Python code for the function body (without the def line)\"\n",
    "                    },\n",
    "                    \"description\": {\n",
    "                        \"type\": \"STRING\",\n",
    "                        \"description\": \"Brief description of what makes this implementation semantically different\"\n",
    "                    },\n",
    "                    \"imports\": {\n",
    "                        \"type\": \"ARRAY\",\n",
    "                        \"items\": {\"type\": \"STRING\"},\n",
    "                        \"description\": \"Any additional imports needed\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"body\", \"description\", \"imports\"]\n",
    "            },\n",
    "            \"description\": \"Array of 4 semantically different implementations\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"implementations\"]\n",
    "}\n",
    "\n",
    "function_body_json = None\n",
    "function_body_raw = \"\"\n",
    "function_body_error = None\n",
    "valid_implementations = []  # Cache for valid implementations\n",
    "MAX_RETRIES = 5\n",
    "\n",
    "# Only proceed if we have signature\n",
    "if pbt_signature_json and not pbt_signature_error and properties_json:\n",
    "    property_list = properties_json.get(\"result\", [])\n",
    "    problem_spec_text = _read_problem_spec(problem=PROBLEM, freeform_spec=FREEFORM_SPEC).strip()\n",
    "    \n",
    "    # Build the function signature string for context\n",
    "    params = pbt_signature_json.get(\"parameters\", [])\n",
    "    if params:\n",
    "        param_strs = [f\"{p['name']}: {p['type_hint']}\" for p in params]\n",
    "        signature_for_prompt = f\"def test_function_pbt({', '.join(param_strs)}) -> None:\"\n",
    "    else:\n",
    "        signature_for_prompt = \"def test_function_pbt() -> None:\"\n",
    "    \n",
    "    properties_str = \"\\n\".join([f\"{i+1}. {prop}\" for i, prop in enumerate(property_list)])\n",
    "    \n",
    "    FUNCTION_BODY_PROMPT = '\\n'.join([\n",
    "        \"You are implementing FOUR semantically different versions of a property-based test function.\",\n",
    "        \"Output MUST be valid JSON and NOTHING ELSE and match the provided schema.\",\n",
    "        \"\",\n",
    "        \"Problem specification:\",\n",
    "        problem_spec_text,\n",
    "        \"\",\n",
    "        \"Properties to test:\",\n",
    "        properties_str,\n",
    "        \"\",\n",
    "        \"Function signature:\",\n",
    "        signature_for_prompt,\n",
    "        \"\",\n",
    "        \"Task:\",\n",
    "        \"Generate FOUR semantically different implementations of the test function body.\",\n",
    "        \"Each implementation MUST:\",\n",
    "        \"1) Be syntactically valid Python code (indented with 4 spaces).\",\n",
    "        \"2) Test some subset of the properties using assertions.\",\n",
    "        \"3) Return None (Hypothesis requirement - no return statement needed).\",\n",
    "        \"4) Print the input values at the start.\",\n",
    "        \"5) Be SEMANTICALLY DIFFERENT from the others in meaningful ways:\",\n",
    "        \"   - Test different subsets of properties\",\n",
    "        \"   - Use different testing strategies (direct vs indirect)\",\n",
    "        \"   - Check different edge cases\",\n",
    "        \"   - Use different assertion styles\",\n",
    "        \"   - Make different assumptions about valid inputs\",\n",
    "        \"\",\n",
    "        \"The differences should be significant enough that when tested with the same\",\n",
    "        \"Hypothesis strategy, the implementations may produce DIFFERENT results (pass/fail).\",\n",
    "        \"\",\n",
    "        \"Important formatting:\",\n",
    "        \"- Return ONLY the function body code (NOT the 'def' line).\",\n",
    "        \"- Use exactly 4 spaces for indentation.\",\n",
    "        \"- Each implementation should be complete and executable.\",\n",
    "        \"- Do NOT use placeholder comments like '# ... rest of implementation'.\",\n",
    "        \"\",\n",
    "        \"Return a JSON object with:\",\n",
    "        '{\"implementations\": [',\n",
    "        '  {\"body\": <string>, \"description\": <string>, \"imports\": [<strings>]},',\n",
    "        '  ... (4 total)',\n",
    "        ']}',\n",
    "    ])\n",
    "    \n",
    "    retry_count = 0\n",
    "    while len(valid_implementations) < 4 and retry_count < MAX_RETRIES:\n",
    "        if retry_count > 0:\n",
    "            print(f\"\\n🔄 Retry attempt {retry_count}/{MAX_RETRIES}: Need {4 - len(valid_implementations)} more valid implementations\")\n",
    "        \n",
    "        try:\n",
    "            function_body_json, function_body_raw = call_gemini_json(\n",
    "                FUNCTION_BODY_PROMPT,\n",
    "                model_name=MODEL_NAME,\n",
    "                temperature=TEMPERATURE,\n",
    "                max_output_tokens=MAX_OUTPUT_TOKENS,\n",
    "                timeout_s=TIMEOUT_SEC,\n",
    "                schema=FUNCTION_BODY_SCHEMA,\n",
    "            )\n",
    "            \n",
    "            implementations = function_body_json.get(\"implementations\", [])\n",
    "            \n",
    "            # Validate each implementation by trying to compile it\n",
    "            for idx, impl in enumerate(implementations):\n",
    "                # Skip if we already have 4 valid ones\n",
    "                if len(valid_implementations) >= 4:\n",
    "                    break\n",
    "                \n",
    "                body = impl.get(\"body\", \"\")\n",
    "                description = impl.get(\"description\", \"\")\n",
    "                imports = impl.get(\"imports\", [])\n",
    "                \n",
    "                # Try to compile the complete function\n",
    "                complete_func = f\"{signature_for_prompt}\\n{body}\"\n",
    "                \n",
    "                try:\n",
    "                    compile(complete_func, '<string>', 'exec')\n",
    "                    # Valid! Add to cache\n",
    "                    valid_implementations.append({\n",
    "                        \"body\": body,\n",
    "                        \"description\": description,\n",
    "                        \"imports\": imports,\n",
    "                        \"index\": len(valid_implementations) + 1\n",
    "                    })\n",
    "                    print(f\"✅ Implementation {len(valid_implementations)}: Valid - {description}\")\n",
    "                except SyntaxError as e:\n",
    "                    print(f\"❌ Implementation candidate {idx+1} invalid: {e}\")\n",
    "                    print(f\"   Description: {description}\")\n",
    "            \n",
    "            retry_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            function_body_error = f\"{type(e).__name__}: {e}\"\n",
    "            print(f\"❌ Error calling LLM: {function_body_error}\")\n",
    "            retry_count += 1\n",
    "    \n",
    "    # Check if we got all 4\n",
    "    if len(valid_implementations) < 4:\n",
    "        function_body_error = f\"Only generated {len(valid_implementations)}/4 valid implementations after {MAX_RETRIES} attempts\"\n",
    "        print(f\"\\n❌ FINAL ERROR: {function_body_error}\")\n",
    "    else:\n",
    "        print(f\"\\n✅ Successfully generated 4 valid implementations!\")\n",
    "        function_body_json = {\"implementations\": valid_implementations}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Generated function body implementations:\")\n",
    "if valid_implementations:\n",
    "    for impl in valid_implementations:\n",
    "        print(f\"\\n--- Implementation {impl['index']} ---\")\n",
    "        print(f\"Description: {impl['description']}\")\n",
    "        print(f\"Imports: {impl['imports'] if impl['imports'] else '(none)'}\")\n",
    "        print(\"Body preview (first 200 chars):\")\n",
    "        print(impl['body'][:200] + \"...\" if len(impl['body']) > 200 else impl['body'])\n",
    "else:\n",
    "    print(\"No valid implementations generated\")\n",
    "    if function_body_error:\n",
    "        print(f\"ERROR: {function_body_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5eba9fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Assembling test functions...\n",
      "============================================================\n",
      "\n",
      "--- Implementation 1 ---\n",
      "Description: This implementation performs a comprehensive check for valid inputs, covering roundtrip conversion, type, character validity, length, repetition limits, and specific invalid subtractive pairs. It also verifies the non-increasing order of token values, ensuring adherence to the greedy decomposition algorithm.\n",
      "  Executing imports:\n",
      "    ROMAN_MAP_VALUES = {\n",
      "    'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000,\n",
      "    'IV': 4, 'IX': 9, 'XL': 40, 'XC': 90, 'CD': 400, 'CM': 900\n",
      "}\n",
      "    ROMAN_SYMBOLS = set('IVXLCDM')\n",
      "    ROMAN_TOKEN_VALUES = [\n",
      "    (1000, 'M'), (900, 'CM'), (500, 'D'), (400, 'CD'), (100, 'C'), (90, 'XC'),\n",
      "    (50, 'L'), (40, 'XL'), (10, 'X'), (9, 'IX'), (5, 'V'), (4, 'IV'), (1, 'I')\n",
      "]\n",
      "    def from_numerals(roman_numeral_string: str) -> int:\n",
      "    if not isinstance(roman_numeral_string, str):\n",
      "        raise TypeError(\"Input must be a string.\")\n",
      "    if not roman_numeral_string:\n",
      "        raise ValueError(\"Input string cannot be empty.\")\n",
      "    total = 0\n",
      "    i = 0\n",
      "    while i < len(roman_numeral_string):\n",
      "        if i + 1 < len(roman_numeral_string):\n",
      "            two_char_symbol = roman_numeral_string[i:i+2]\n",
      "            if two_char_symbol in ROMAN_MAP_VALUES and len(two_char_symbol) == 2:\n",
      "                total += ROMAN_MAP_VALUES[two_char_symbol]\n",
      "                i += 2\n",
      "                continue\n",
      "        one_char_symbol = roman_numeral_string[i]\n",
      "        if one_char_symbol in ROMAN_MAP_VALUES:\n",
      "            total += ROMAN_MAP_VALUES[one_char_symbol]\n",
      "            i += 1\n",
      "        else:\n",
      "            raise ValueError(f\"Invalid Roman numeral symbol: {one_char_symbol}\")\n",
      "    return total\n",
      "    def get_token_values(roman_string: str) -> list[int]:\n",
      "    values = []\n",
      "    i = 0\n",
      "    while i < len(roman_string):\n",
      "        matched = False\n",
      "        for val, token in ROMAN_TOKEN_VALUES:\n",
      "            if roman_string.startswith(token, i):\n",
      "                values.append(val)\n",
      "                i += len(token)\n",
      "                matched = True\n",
      "                break\n",
      "        if not matched:\n",
      "            raise ValueError(f\"Could not parse token at index {i} in {roman_string}\")\n",
      "    return values\n",
      "  ✅ Function test_function_pbt_1 created successfully\n",
      "\n",
      "--- Implementation 2 ---\n",
      "Description: This implementation focuses primarily on error handling for invalid input types and out-of-range integer values, asserting that the correct exceptions (TypeError or ValueError) are raised. It also includes direct assertions for specific, well-known Roman numeral conversions.\n",
      "  Executing imports:\n",
      "    ROMAN_MAP_VALUES = {\n",
      "    'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000,\n",
      "    'IV': 4, 'IX': 9, 'XL': 40, 'XC': 90, 'CD': 400, 'CM': 900\n",
      "}\n",
      "    ROMAN_SYMBOLS = set('IVXLCDM')\n",
      "    ROMAN_TOKEN_VALUES = [\n",
      "    (1000, 'M'), (900, 'CM'), (500, 'D'), (400, 'CD'), (100, 'C'), (90, 'XC'),\n",
      "    (50, 'L'), (40, 'XL'), (10, 'X'), (9, 'IX'), (5, 'V'), (4, 'IV'), (1, 'I')\n",
      "]\n",
      "    def from_numerals(roman_numeral_string: str) -> int:\n",
      "    if not isinstance(roman_numeral_string, str):\n",
      "        raise TypeError(\"Input must be a string.\")\n",
      "    if not roman_numeral_string:\n",
      "        raise ValueError(\"Input string cannot be empty.\")\n",
      "    total = 0\n",
      "    i = 0\n",
      "    while i < len(roman_numeral_string):\n",
      "        if i + 1 < len(roman_numeral_string):\n",
      "            two_char_symbol = roman_numeral_string[i:i+2]\n",
      "            if two_char_symbol in ROMAN_MAP_VALUES and len(two_char_symbol) == 2:\n",
      "                total += ROMAN_MAP_VALUES[two_char_symbol]\n",
      "                i += 2\n",
      "                continue\n",
      "        one_char_symbol = roman_numeral_string[i]\n",
      "        if one_char_symbol in ROMAN_MAP_VALUES:\n",
      "            total += ROMAN_MAP_VALUES[one_char_symbol]\n",
      "            i += 1\n",
      "        else:\n",
      "            raise ValueError(f\"Invalid Roman numeral symbol: {one_char_symbol}\")\n",
      "    return total\n",
      "    def get_token_values(roman_string: str) -> list[int]:\n",
      "    values = []\n",
      "    i = 0\n",
      "    while i < len(roman_string):\n",
      "        matched = False\n",
      "        for val, token in ROMAN_TOKEN_VALUES:\n",
      "            if roman_string.startswith(token, i):\n",
      "                values.append(val)\n",
      "                i += len(token)\n",
      "                matched = True\n",
      "                break\n",
      "        if not matched:\n",
      "            raise ValueError(f\"Could not parse token at index {i} in {roman_string}\")\n",
      "    return values\n",
      "    EXPECTED_VALUES = {\n",
      "        1: 'I', 4: 'IV', 5: 'V', 9: 'IX', 10: 'X', 40: 'XL', 50: 'L', 90: 'XC',\n",
      "        100: 'C', 400: 'CD', 500: 'D', 900: 'CM', 1000: 'M'\n",
      "    }\n",
      "  ✅ Function test_function_pbt_2 created successfully\n",
      "\n",
      "--- Implementation 3 ---\n",
      "Description: This implementation emphasizes structural correctness of the Roman numeral string, performing extensive substring checks to ensure adherence to repetition limits (e.g., no 'VV', 'IIII') and the absence of invalid two-symbol sequences. It also verifies that any subtractive pairs are among the allowed canonical forms.\n",
      "  Executing imports:\n",
      "    ROMAN_MAP_VALUES = {\n",
      "    'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000,\n",
      "    'IV': 4, 'IX': 9, 'XL': 40, 'XC': 90, 'CD': 400, 'CM': 900\n",
      "}\n",
      "    ROMAN_SYMBOLS = set('IVXLCDM')\n",
      "    ROMAN_TOKEN_VALUES = [\n",
      "    (1000, 'M'), (900, 'CM'), (500, 'D'), (400, 'CD'), (100, 'C'), (90, 'XC'),\n",
      "    (50, 'L'), (40, 'XL'), (10, 'X'), (9, 'IX'), (5, 'V'), (4, 'IV'), (1, 'I')\n",
      "]\n",
      "    def from_numerals(roman_numeral_string: str) -> int:\n",
      "    if not isinstance(roman_numeral_string, str):\n",
      "        raise TypeError(\"Input must be a string.\")\n",
      "    if not roman_numeral_string:\n",
      "        raise ValueError(\"Input string cannot be empty.\")\n",
      "    total = 0\n",
      "    i = 0\n",
      "    while i < len(roman_numeral_string):\n",
      "        if i + 1 < len(roman_numeral_string):\n",
      "            two_char_symbol = roman_numeral_string[i:i+2]\n",
      "            if two_char_symbol in ROMAN_MAP_VALUES and len(two_char_symbol) == 2:\n",
      "                total += ROMAN_MAP_VALUES[two_char_symbol]\n",
      "                i += 2\n",
      "                continue\n",
      "        one_char_symbol = roman_numeral_string[i]\n",
      "        if one_char_symbol in ROMAN_MAP_VALUES:\n",
      "            total += ROMAN_MAP_VALUES[one_char_symbol]\n",
      "            i += 1\n",
      "        else:\n",
      "            raise ValueError(f\"Invalid Roman numeral symbol: {one_char_symbol}\")\n",
      "    return total\n",
      "    def get_token_values(roman_string: str) -> list[int]:\n",
      "    values = []\n",
      "    i = 0\n",
      "    while i < len(roman_string):\n",
      "        matched = False\n",
      "        for val, token in ROMAN_TOKEN_VALUES:\n",
      "            if roman_string.startswith(token, i):\n",
      "                values.append(val)\n",
      "                i += len(token)\n",
      "                matched = True\n",
      "                break\n",
      "        if not matched:\n",
      "            raise ValueError(f\"Could not parse token at index {i} in {roman_string}\")\n",
      "    return values\n",
      "  ✅ Function test_function_pbt_3 created successfully\n",
      "\n",
      "--- Implementation 4 ---\n",
      "Description: This implementation prioritizes the core functional correctness by asserting the roundtrip property (from_numerals(to_numerals(N)) == N). It also strongly verifies the algorithmic behavior by checking that the parsed token values are strictly non-increasing, reflecting the greedy decomposition requirement.\n",
      "  Executing imports:\n",
      "    ROMAN_MAP_VALUES = {\n",
      "    'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000,\n",
      "    'IV': 4, 'IX': 9, 'XL': 40, 'XC': 90, 'CD': 400, 'CM': 900\n",
      "}\n",
      "    ROMAN_SYMBOLS = set('IVXLCDM')\n",
      "    ROMAN_TOKEN_VALUES = [\n",
      "    (1000, 'M'), (900, 'CM'), (500, 'D'), (400, 'CD'), (100, 'C'), (90, 'XC'),\n",
      "    (50, 'L'), (40, 'XL'), (10, 'X'), (9, 'IX'), (5, 'V'), (4, 'IV'), (1, 'I')\n",
      "]\n",
      "    def from_numerals(roman_numeral_string: str) -> int:\n",
      "    if not isinstance(roman_numeral_string, str):\n",
      "        raise TypeError(\"Input must be a string.\")\n",
      "    if not roman_numeral_string:\n",
      "        raise ValueError(\"Input string cannot be empty.\")\n",
      "    total = 0\n",
      "    i = 0\n",
      "    while i < len(roman_numeral_string):\n",
      "        if i + 1 < len(roman_numeral_string):\n",
      "            two_char_symbol = roman_numeral_string[i:i+2]\n",
      "            if two_char_symbol in ROMAN_MAP_VALUES and len(two_char_symbol) == 2:\n",
      "                total += ROMAN_MAP_VALUES[two_char_symbol]\n",
      "                i += 2\n",
      "                continue\n",
      "        one_char_symbol = roman_numeral_string[i]\n",
      "        if one_char_symbol in ROMAN_MAP_VALUES:\n",
      "            total += ROMAN_MAP_VALUES[one_char_symbol]\n",
      "            i += 1\n",
      "        else:\n",
      "            raise ValueError(f\"Invalid Roman numeral symbol: {one_char_symbol}\")\n",
      "    return total\n",
      "    def get_token_values(roman_string: str) -> list[int]:\n",
      "    values = []\n",
      "    i = 0\n",
      "    while i < len(roman_string):\n",
      "        matched = False\n",
      "        for val, token in ROMAN_TOKEN_VALUES:\n",
      "            if roman_string.startswith(token, i):\n",
      "                values.append(val)\n",
      "                i += len(token)\n",
      "                matched = True\n",
      "                break\n",
      "        if not matched:\n",
      "            raise ValueError(f\"Could not parse token at index {i} in {roman_string}\")\n",
      "    return values\n",
      "  ✅ Function test_function_pbt_4 created successfully\n",
      "\n",
      "============================================================\n",
      "Created 4 test function implementations\n",
      "============================================================\n",
      "\n",
      "Function signatures and strategies:\n",
      "Parameters: ['N: Any']\n",
      "\n",
      "Strategies:\n",
      "  N: st.one_of(st.integers(min_value=1, max_value=3999), st.integers(max_value=0), st.integers(min_value=4000), st.floats(allow_nan=false, allow_infinity=false), st.text(), st.none(), st.booleans()) # Generates valid integers (1-3999) for core functionality tests, out-of-range integers (<1, >3999) for ValueError tests, and various non-integer types (float, string, None, boolean) for TypeError tests. This comprehensive strategy allows a single test function to cover all specified properties, including error handling.\n",
      "\n",
      "Available functions:\n",
      "  - test_function_pbt_1: This implementation performs a comprehensive check for valid inputs, covering roundtrip conversion, type, character validity, length, repetition limits, and specific invalid subtractive pairs. It also verifies the non-increasing order of token values, ensuring adherence to the greedy decomposition algorithm.\n",
      "  - test_function_pbt_2: This implementation focuses primarily on error handling for invalid input types and out-of-range integer values, asserting that the correct exceptions (TypeError or ValueError) are raised. It also includes direct assertions for specific, well-known Roman numeral conversions.\n",
      "  - test_function_pbt_3: This implementation emphasizes structural correctness of the Roman numeral string, performing extensive substring checks to ensure adherence to repetition limits (e.g., no 'VV', 'IIII') and the absence of invalid two-symbol sequences. It also verifies that any subtractive pairs are among the allowed canonical forms.\n",
      "  - test_function_pbt_4: This implementation prioritizes the core functional correctness by asserting the roundtrip property (from_numerals(to_numerals(N)) == N). It also strongly verifies the algorithmic behavior by checking that the parsed token values are strictly non-increasing, reflecting the greedy decomposition requirement.\n"
     ]
    }
   ],
   "source": [
    "# ===== Generate and define the test_function_pbt functions =====\n",
    "# Parse the signature and use LLM-generated bodies (4 implementations)\n",
    "\n",
    "test_functions = []  # List to store all generated functions\n",
    "\n",
    "if pbt_signature_json and not pbt_signature_error and function_body_json and not function_body_error:\n",
    "    params = pbt_signature_json.get(\"parameters\", [])\n",
    "    strategies = pbt_signature_json.get(\"strategies\", [])\n",
    "    implementations = function_body_json.get(\"implementations\", [])\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"Assembling test functions...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for impl in implementations:\n",
    "        idx = impl['index']\n",
    "        body_code = impl['body']\n",
    "        imports = impl.get('imports', [])\n",
    "        description = impl['description']\n",
    "        \n",
    "        print(f\"\\n--- Implementation {idx} ---\")\n",
    "        print(f\"Description: {description}\")\n",
    "        \n",
    "        # Build the function signature with index\n",
    "        if params:\n",
    "            param_strs = [f\"{p['name']}: {p['type_hint']}\" for p in params]\n",
    "            signature_str = f\"def test_function_pbt_{idx}({', '.join(param_strs)}) -> None:\"\n",
    "        else:\n",
    "            signature_str = f\"def test_function_pbt_{idx}() -> None:\"\n",
    "        \n",
    "        # Ensure body is properly indented\n",
    "        if not body_code.startswith(\"    \"):\n",
    "            body_lines = body_code.split('\\n')\n",
    "            body_code = '\\n'.join(['    ' + line if line.strip() else '' for line in body_lines])\n",
    "        \n",
    "        function_code = f'''{signature_str}\n",
    "    \"\"\"Property-based test function generated by LLM.\n",
    "    \n",
    "    {description}\n",
    "    \"\"\"\n",
    "{body_code}\n",
    "'''\n",
    "        \n",
    "        # Execute any needed imports first\n",
    "        if imports:\n",
    "            print(f\"  Executing imports:\")\n",
    "            for imp in imports:\n",
    "                print(f\"    {imp}\")\n",
    "                try:\n",
    "                    exec(imp, globals())\n",
    "                except Exception as e:\n",
    "                    print(f\"      Warning: {e}\")\n",
    "        \n",
    "        # Execute the function definition to make it available\n",
    "        try:\n",
    "            exec(function_code, globals())\n",
    "            print(f\"  ✅ Function test_function_pbt_{idx} created successfully\")\n",
    "            \n",
    "            # Store the function info\n",
    "            test_functions.append({\n",
    "                \"index\": idx,\n",
    "                \"description\": description,\n",
    "                \"code\": function_code,\n",
    "                \"imports\": imports,\n",
    "                \"function_name\": f\"test_function_pbt_{idx}\"\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error creating function: {e}\")\n",
    "            print(f\"  Code was:\\n{function_code}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Created {len(test_functions)} test function implementations\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Print summary\n",
    "    if test_functions:\n",
    "        print(\"\\nFunction signatures and strategies:\")\n",
    "        print(f\"Parameters: {param_strs if params else 'None'}\")\n",
    "        print(f\"\\nStrategies:\")\n",
    "        for s in strategies:\n",
    "            notes_str = f\" # {s.get('notes')}\" if s.get('notes') else \"\"\n",
    "            print(f\"  {s['param_name']}: {s['strategy']}{notes_str}\")\n",
    "        \n",
    "        print(\"\\nAvailable functions:\")\n",
    "        for tf in test_functions:\n",
    "            print(f\"  - {tf['function_name']}: {tf['description']}\")\n",
    "else:\n",
    "    print(\"Skipping function generation due to errors or missing data\")\n",
    "    if pbt_signature_error:\n",
    "        print(f\"Signature error: {pbt_signature_error}\")\n",
    "    if function_body_error:\n",
    "        print(f\"Body error: {function_body_error}\")\n",
    "    test_functions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "106535f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing all generated test_function_pbt implementations with sample inputs:\n",
      "\n",
      "\n",
      "test_function_pbt_1: This implementation performs a comprehensive check for valid inputs, covering roundtrip conversion, type, character validity, length, repetition limits, and specific invalid subtractive pairs. It also verifies the non-increasing order of token values, ensuring adherence to the greedy decomposition algorithm.\n",
      "(Call with appropriate arguments based on the signature)\n",
      "\n",
      "test_function_pbt_2: This implementation focuses primarily on error handling for invalid input types and out-of-range integer values, asserting that the correct exceptions (TypeError or ValueError) are raised. It also includes direct assertions for specific, well-known Roman numeral conversions.\n",
      "(Call with appropriate arguments based on the signature)\n",
      "\n",
      "test_function_pbt_3: This implementation emphasizes structural correctness of the Roman numeral string, performing extensive substring checks to ensure adherence to repetition limits (e.g., no 'VV', 'IIII') and the absence of invalid two-symbol sequences. It also verifies that any subtractive pairs are among the allowed canonical forms.\n",
      "(Call with appropriate arguments based on the signature)\n",
      "\n",
      "test_function_pbt_4: This implementation prioritizes the core functional correctness by asserting the roundtrip property (from_numerals(to_numerals(N)) == N). It also strongly verifies the algorithmic behavior by checking that the parsed token values are strictly non-increasing, reflecting the greedy decomposition requirement.\n",
      "(Call with appropriate arguments based on the signature)\n"
     ]
    }
   ],
   "source": [
    "# ===== Test the generated functions with sample inputs =====\n",
    "\n",
    "if test_functions:\n",
    "    print(\"Testing all generated test_function_pbt implementations with sample inputs:\\n\")\n",
    "    \n",
    "    # Try calling with some sample values based on the parameters\n",
    "    if pbt_signature_json:\n",
    "        params = pbt_signature_json.get(\"parameters\", [])\n",
    "        \n",
    "        if params:\n",
    "            # For roman_numerals, we expect an integer parameter\n",
    "            if any('int' in p.get('type_hint', '').lower() for p in params):\n",
    "                test_values = [1, 10, 100, 3999]\n",
    "                \n",
    "                for tf in test_functions:\n",
    "                    func_name = tf['function_name']\n",
    "                    func = globals().get(func_name)\n",
    "                    \n",
    "                    if func:\n",
    "                        print(\"=\"*60)\n",
    "                        print(f\"Testing: {func_name}\")\n",
    "                        print(f\"Description: {tf['description']}\")\n",
    "                        print(\"-\" * 60)\n",
    "                        \n",
    "                        for test_val in test_values:\n",
    "                            try:\n",
    "                                result = func(test_val)\n",
    "                                print(f\"✅ test_val={test_val}: Passed\")\n",
    "                            except AssertionError as e:\n",
    "                                print(f\"❌ test_val={test_val}: Failed - {e}\")\n",
    "                            except Exception as e:\n",
    "                                print(f\"⚠️  test_val={test_val}: Error - {e}\")\n",
    "                        print()\n",
    "            else:\n",
    "                # Generic test\n",
    "                for tf in test_functions:\n",
    "                    func_name = tf['function_name']\n",
    "                    print(f\"\\n{func_name}: {tf['description']}\")\n",
    "                    print(\"(Call with appropriate arguments based on the signature)\")\n",
    "        else:\n",
    "            # No parameters\n",
    "            for tf in test_functions:\n",
    "                func_name = tf['function_name']\n",
    "                func = globals().get(func_name)\n",
    "                if func:\n",
    "                    try:\n",
    "                        result = func()\n",
    "                        print(f\"✅ {func_name}: Passed\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"❌ {func_name}: {e}\")\n",
    "else:\n",
    "    print(\"No test functions defined - check for errors in previous cells\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6173eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running all test_function_pbt implementations with Hypothesis...\n",
      "\n",
      "Error evaluating strategy for N: name 'false' is not defined\n",
      "Could not evaluate strategies\n"
     ]
    }
   ],
   "source": [
    "# ===== Run test_function_pbt implementations with Hypothesis =====\n",
    "# This uses the @given decorator to run each function with generated inputs\n",
    "\n",
    "if pbt_signature_json and not pbt_signature_error and test_functions:\n",
    "    from hypothesis import given, settings\n",
    "    \n",
    "    strategies_list = pbt_signature_json.get(\"strategies\", [])\n",
    "    \n",
    "    if strategies_list:\n",
    "        print(\"Running all test_function_pbt implementations with Hypothesis...\\n\")\n",
    "        \n",
    "        # Build the strategy arguments dynamically\n",
    "        strategy_kwargs = {}\n",
    "        for s in strategies_list:\n",
    "            param_name = s['param_name']\n",
    "            strategy_expr = s['strategy']\n",
    "            \n",
    "            # Fix JSON-style booleans (true/false) to Python booleans (True/False)\n",
    "            strategy_expr_fixed = strategy_expr.replace('true', 'True').replace('false', 'False')\n",
    "            \n",
    "            # Evaluate the strategy expression to get the actual strategy object\n",
    "            try:\n",
    "                strategy_obj = eval(strategy_expr_fixed)\n",
    "                strategy_kwargs[param_name] = strategy_obj\n",
    "                print(f\"Strategy for {param_name}: {strategy_expr_fixed}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating strategy for {param_name}: {e}\")\n",
    "                print(f\"  Original expression: {strategy_expr}\")\n",
    "                print(f\"  Fixed expression: {strategy_expr_fixed}\")\n",
    "                strategy_kwargs = None\n",
    "                break\n",
    "        \n",
    "        if strategy_kwargs:\n",
    "            print(f\"\\nRunning 100 test cases per implementation...\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            # Test each implementation\n",
    "            for tf in test_functions:\n",
    "                func_name = tf['function_name']\n",
    "                func = globals().get(func_name)\n",
    "                \n",
    "                if func:\n",
    "                    print(f\"\\n--- Testing: {func_name} ---\")\n",
    "                    print(f\"Description: {tf['description']}\")\n",
    "                    print(\"-\" * 60)\n",
    "                    \n",
    "                    # Create a wrapper that applies @given decorator\n",
    "                    @given(**strategy_kwargs)\n",
    "                    @settings(max_examples=100, print_blob=True)\n",
    "                    def run_test(**kwargs):\n",
    "                        return func(**kwargs)\n",
    "                    \n",
    "                    # Run the test\n",
    "                    try:\n",
    "                        run_test()\n",
    "                        print(f\"✅ All 100 test cases passed for {func_name}!\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"❌ Test failed for {func_name}: {e}\")\n",
    "                    \n",
    "                    print()\n",
    "        else:\n",
    "            print(\"Could not evaluate strategies\")\n",
    "    else:\n",
    "        print(\"No strategies available\")\n",
    "else:\n",
    "    if not test_functions:\n",
    "        print(\"No test functions available - check for errors in previous cells\")\n",
    "    elif pbt_signature_error:\n",
    "        print(f\"Cannot run tests - signature error: {pbt_signature_error}\")\n",
    "    else:\n",
    "        print(\"Cannot run with Hypothesis - missing signature or function not defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "787385f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Run log written to: logs/roman_numerals_20260115_204027Z.json\n",
      "Generated 4 function implementations\n"
     ]
    }
   ],
   "source": [
    "# Write run log to logs/<problem>_<timestamp>.json\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from typing import Any\n",
    "\n",
    "def _jsonable(obj: Any) -> Any:\n",
    "    \"\"\"Best-effort conversion to JSON-serializable types.\"\"\"\n",
    "    try:\n",
    "        json.dumps(obj)\n",
    "        return obj\n",
    "    except TypeError:\n",
    "        return repr(obj)\n",
    "\n",
    "run_log = {\n",
    "    \"timestamp_utc\": RUN_TIMESTAMP,\n",
    "    \"problem\": PROBLEM,\n",
    "    \"settings\": {\n",
    "        \"FREEFORM_SPEC\": FREEFORM_SPEC,\n",
    "        \"MODEL_NAME\": MODEL_NAME,\n",
    "        \"TEMPERATURE\": TEMPERATURE,\n",
    "        \"MAX_OUTPUT_TOKENS\": MAX_OUTPUT_TOKENS,\n",
    "        \"TIMEOUT_SEC\": TIMEOUT_SEC,\n",
    "        \"python_version\": sys.version,\n",
    "        \"platform\": platform.platform(),\n",
    "    },\n",
    "    \"prompts\": {\n",
    "        \"properties_prompt\": PROMPT,\n",
    "        \"signature_strategy_prompt\": SIGNATURE_STRATEGY_PROMPT,\n",
    "        \"function_body_prompt\": FUNCTION_BODY_PROMPT if 'FUNCTION_BODY_PROMPT' in globals() else None,\n",
    "    },\n",
    "    \"responses\": {\n",
    "        \"properties\": {\n",
    "            \"raw\": properties_raw,\n",
    "            \"parsed\": _jsonable(properties_json),\n",
    "            \"error\": properties_error,\n",
    "        },\n",
    "        \"pbt_signature\": {\n",
    "            \"raw\": pbt_signature_raw,\n",
    "            \"parsed\": _jsonable(pbt_signature_json),\n",
    "            \"error\": pbt_signature_error,\n",
    "        },\n",
    "        \"function_body\": {\n",
    "            \"raw\": function_body_raw,\n",
    "            \"parsed\": _jsonable(function_body_json),\n",
    "            \"error\": function_body_error,\n",
    "            \"generated_functions\": [\n",
    "                {\n",
    "                    \"index\": tf[\"index\"],\n",
    "                    \"function_name\": tf[\"function_name\"],\n",
    "                    \"description\": tf[\"description\"],\n",
    "                    \"code\": tf[\"code\"]\n",
    "                }\n",
    "                for tf in test_functions\n",
    "            ] if test_functions else []\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "with RUN_LOG_PATH.open(\"w\") as f:\n",
    "    json.dump(run_log, f, indent=2)\n",
    "\n",
    "print(f\"\\n✅ Run log written to: {RUN_LOG_PATH}\")\n",
    "print(f\"Generated {len(test_functions) if test_functions else 0} function implementations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
